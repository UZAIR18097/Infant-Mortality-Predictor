# -*- coding: utf-8 -*-
"""Data Science Project.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1prif-ssmBEXtv5hdxyHQT4pguSP7YILf
"""

!pip install tensorflow

pd.set_option('display.max_columns', None)
pd.set_option('display.max_rows', None)

import numpy as np 
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from keras.models import Sequential
from keras.layers import Dense
from keras.wrappers.scikit_learn import KerasClassifier
from sklearn.model_selection import cross_val_score
from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import StratifiedKFold
from sklearn.preprocessing import StandardScaler
from sklearn.pipeline import Pipeline
from keras import models
from keras import layers

#Importing the data 
from google.colab import files
files.upload()

#Importing the data 
from google.colab import files
files.upload()

from google.colab import files
files.upload()

"""# The goal of this project is to build a classification model using data set of Paksitan demographic and Health Survey(2017-2018)

## Problem Statement : To predict whether an infant survives or not using the given attributes.

**GOAL : To make a system which can help Doctors to predict infant Performance in next 12 months given attributes of present**

## Data Pipeline:
### 1) Getting the required packages and tools.
### 2) Uploading data of Pakistan Demographic and Health Survey(2017-2018) and understanding it.
### 3) Extracting infants sample points.
### 4) Finding class values and seperating it from data.
### 5) Data Preprocessing(Dropping the irrelvent attributes,Feature cleaning,Null values,Outliers detection and Normalization).
### 6) Dimensionality Reduction using PCA
### 7) Model Buliding and Evaluation(Deep learning,Naive Bayes)
### 8) Model Deployment.

## 1) Packages and Tools
"""

import pandas as pd
import numpy as np
import matplotlib as plt
import seaborn as sns

"""## 2) Uploading Data"""

df,meta = pyreadstat.read_sav("./PKBR71FL.SAV",apply_value_formats = True)
variables = meta.column_labels
values = meta.variable_value_labels
var_labels = meta.variable_to_label
cols=df.columns.tolist()
df.columns = pd.Index(variables)

df.head()

df.describe()

"""## 3) Finding those sample points who were infants(less than 12 months old)"""

import datetime as dt
#attribute 524 = Month of birth
#attribute 525 = Year of birth
#attribute 528 = Child is alive
#attribute 06 = Month of interview
#attribute 07 = Year of interview
dfe = pd.DataFrame()
for i in range(len(df["Child is alive"])):
    Dob = dt.date(int(df["Year of birth"][i]),int(df["Month of birth"][i]),1)
    Doc = dt.date(int(df["Year of interview"][i]),int(df["Month of interview"][i]),1)
    diff = (Dob-Doc).days
    if abs(diff)<365:
        dfe=dfe.append(df[i:i+1])
dfe = dfe.reset_index(drop = True)
dfe.csv("C:\Users\uzeea")

"""### Uploading the file with only infants"""

df = pd.read_csv("infants first_copy.csv",low_memory = False)

print(df.dtypes)
c = df.dtypes == "object"
d = df.dtypes == "float64"
print(c.sum()) #697
print(d.sum()) #488

df.head()

"""## 4) Finding class values and storing them in Y"""

df["B5"]

#B5 = "Child is alive"
dfv = df['B5'].value_counts(dropna=False)
dfv

#finding how many class values are null
c = df["B5"].isnull().sum()
print(c)

#replacing Yes with 1 and 0 with No 
df['B5'] = df['B5'].map({'Yes': 1, 'No': 0})
df.B5

X = df.loc[:, df.columns != 'B5']
Y = df.iloc[:,528]
dfx,dfy = pd.DataFrame(X),pd.DataFrame(Y)
dfy.columns = ["target"]

dfx.head()

dfy.head()

dfy.to_csv('target.csv')

"""## 5) Data Preprocessing

## i) Checking Data types
"""

pd.set_option('display.max_rows', None)
pd.set_option('display.max_columns', None)
pd.set_option('display.width', None)
pd.set_option('display.max_colwidth', -1)

print(dfx.dtypes)
c = dfx.dtypes == "object"
d = dfx.dtypes == "float64"
print(c.sum()) #697
print(d.sum()) #488

"""## ii) Dropping irrelevant columns

### Since the data has 1186 features and 2486 samples now, I have to drop some features. Following criteria will be used to drop features:
#### 1) Features that are random generated numbers like CaseID,Country code and Phase etc
#### 2) Features that has more than 60 % of nan values
#### 3) Features that leaks information from future such as (age of death,reason of death etc)
#### 4) Features that give redundant information so my model will not learn anything new from them.
#### 5)Features which require extensive feature engineering such as date columns etc.

## I have made a seprate excel file for all features,their labels,data type and Number of missing value in each feature which is uploaded below
"""

des = pd.read_excel("name_labels.xlsx")
des = des.drop(["Unnamed: 3","Unnamed: 4"],axis = 1)
des.columns = ["Variable","Label description","datatype","Number of missing values"]
des.head()

"""## Since it is very cumbersome to analyze all the 1186 features at same time I will be analyzing at 100 features at a time"""

#analyzing first 100 features 
des.iloc[:100]

#selecting and dropping from first 100
dfx = dfx.drop(["Unnamed: 0","CASEID","BIDX","V000","V001","V002","V003","V004","V006","V007","V008","V008A","V009","V010","V011","V013","V014","V015","V016","V017","V018","V019","V019A","V020","V021","V022","V023","V026","V027","V028","V029","V030","V031","V032","V034","V040","V044","V045A","V045B","V045C","V046","V101","V102","V103","V104","V105","V105A","V107","V116","V130","V131","V133","V134","V139","V140","V141","V149","V150","V156","V166","V167","V168","V169B"],axis = 1)

dfx.shape
#dropped 63 variables from first 100

#analyzing next second 100 features
des.iloc[100:200]

#selecting and dropping from second 100
dfx = dfx.drop(["V171B","V190","V190A","V202","V203","V204","V205","V208","V210","V211","V214","V215","V218","V220","V222","V223","V225","V226","V227","V229","V230","V231","V232","V233","V234","V237","V238","V239","V240","V241","V242","V243","V310","V311","V312","V315","V316","V317","V318","V319","V320","V321","V322","V323","V323A","V325A","V326","V327","V337","V359","V360","V362","V363","V362","V372","V372A","V375A","V376","V376A","V379","V380","V3A00A","V3A00B","V3A00C","V3A00D","V3A00E","V3A00F","V3A00G","V3A00H","V3A00I","V3A00J"],axis = 1)

dfx.shape
#dropped 70 features from second 100

#analyzing third 100 features
des.iloc[200:300]

#selecting and dropping from third 100
dfx = dfx.drop(["V3A00K","V3A00L","V3A00M","V3A00N","V3A00O","V3A00P","V3A00Q","V3A00R","V3A00S","V3A00T","V3A00U","V3A00V","V3A00W","V3A00X","V3A00Y","V3A00Z","V3A01","V3A02","V3A03","V3A04","V3A05","V3A06","V3A07","V3A08A","V3A08B","V3A08C","V3A08D","V3A08E","V3A08F","V3A08G","V3A08H","V3A08I","V3A08J","V3A08K","V3A08L","V3A08M","V3A08N","V3A08O","V3A08P","V3A08Q","V3A08R","V3A08S","V3A08T","V3A08U","V3A08V","V3A08W","V3A08AA","V3A08AB","V3A08AC","V3A08AD","V3A08X","V3A08Z","V3A09A","V3A09B","V407","V408","V409A","V410A","V412","V412B","V413A","V413B","V413C","V413D","V414A","V414B","V414C","V414D","V414Q","V414R","V414S","V414T","V414U","V414V","V414W","V415"],axis =1)

dfx.shape
#dropped 76 features from third 100

#analyzing fourth 100 features
des.iloc[300:400]

#electing and dropping from fourth 100 features
dfx = dfx.drop(["V420","V421","V452A","V452B","V452C","V453","V454","V455","V456","V457","V458","V462","V463E","V463F","V463J","V463K","V463L","V464","V465","V466","V467A","V467B","V467C","V467D","V467E","V467F","V467G","V467H","V467I","V467J","V467K","V467L","V467M","V468","V469E","V469F","V469X","V471A","V471B","V471C","V471D","V471E","V471F","V471G","V472A","V472B","V472C","V472E","V472F","V472G","V472H","V472I","V472J","V472K","V472L","V472M","V472N","V472O","V472P","V472Q","V472R","V472S","V472T","V472U","V473A","V473B"],axis = 1)

dfx.shape
#dropped 66 features from fourth 100 features

#analyzing fifth 100 features
des.iloc[400:500]

#selecting and dropping fifth 100 features
dfx = dfx.drop(["V474A","V474B","V474C","V474D","V474E","V474F","V474G","V474H","V474I","V474J","V474X","V474Z","V475","V476","V477","V478","V479","V480","V481","V481A","V481B","V481C","V481D","V481E","V481F","V481G","V481H","V481X","V482A","V482B","V482C","V501","V502","V503","V504","V505","V506","V507","V508","V509","V510","V512","V513","V527","V528","V529","V530","V531","V532","V535","V537","V538","V539","V540","V541","V603","V604","V613","V616","V626","V625A","V626A","V627","V628","V629","V631","V632","V633A","V633B","V633C","V633D","V633E","V633F","V633G","V634","V702","V704","V704A","V714A","V715","V716","V719","V721","V729"],axis = 1)

dfx.shape
#dropped 84 features from fifth 100 features

#analyzing sixth 100 features
des.iloc[500:600]

#selecting and dropping sixth 100 features
dfx = dfx.drop(["V731","V732","V739","V740","V741","V743A","V743B","V743C","V743D","V743E","V743F","V744A","V744B","V744C","V744D","V744E","V745A","V745B","V745C","V745D","V746","B1","B2","B3","B6","B7","B8","B9","B10","B12","B13","B17","B18","B19","B20","M1B","M1C","M1D","M1E","M2C","M2D","M2E","M2F","M2M","M3F","M2K","M2I","M2J","M2M","M3H","M3J","M3K","M3L","M3N","M11","M17A","M19A","M27","M28","M29","M35","M36",],axis =1)

dfx.shape
#dropped 61 features from sixth 100 features

#analyzing the seventh 100 features
des.iloc[600:700]

#selecting and dropping from seventh 100 features 
dfx = dfx.drop(["M42A","M42B","M43","M44","M47","M48","M49A","M49B","M49C","M49D","M49E","M49F","M49G","M49X","M49Z","M49Y","M54","M55A","M55B","M55C","M55D","M55E","M55F","M55G","M55H","M55I","M55J","M55K","M55L","M55M","M55N","M55O","M55X","M55Z","M57C","M57D","M57I","M57J","M57K","M57L","M57Q","M57R","M57S","M57T","M57U","M57V","M57X","M61","M63","M65A","M65B","M65C","M65D","M65E","M65F","M65G","M65H","M65I","M65J","M65K","M65L","M65X","M66","M67","M68","M69","M71","M72","M73","M75","M76","M78F"],axis =1)

dfx.shape
#dropped 72 features from seventh 100 features

#analyzing eight 100 features 
des.iloc[700:800]

#selecting and dropping eight 100 features
dfx = dfx.drop(["M78G","M78H","M78I","M78J","HIDX","H1","H1A","H2D","H2M","H2Y","H3D","H3M","H3Y","H4D","H4M","H4Y","H5D","H5Y","H5M","H6D","H6M","H6Y","H7D","H7M","H7Y","H8D","H8M","H8Y","H9D","H9M","H9Y","H9AD","H9AM","H9AY","H0D","H0M","H0Y","H33D","H33M","H33Y","H35","H36A","H36B","H36C","H36D","H36E","H36F","H40","H40D","H40M","H40Y","H41A","H41B","H50","H50D","H50M","H50Y","H51","H51D","H51M","H51Y","H52D","H52M","H52Y","H53D","H53M","H53Y","H54D","H54M","H54Y","H56D","H56M","H56Y","H57","H57D","H57M","H57Y","H58","H58D","H58M",],axis=1)

dfx.shape
#dropped eighty from eighth 100 features

#analyzing ninth 100 features
des.iloc[800:900]

#selecting and dropping from ninth 100
dfx = dfx.drop(["H58Y","H59","H59D","H59M","H59Y","H60D","H60M","H60Y","H61D","H61M","H61Y","H62D","H62M","H62Y","H63D","H63M","H63Y","H64D","H64M","H64Y","H65D","H65M","H65Y","H66D","H66M","H66Y","H80A","H80B","H80C","H80D","H80E","H80F","H80G","H11B","H12A","H12B","H12C","H12D","H12E","H12F","H12G","H12H","H12I","H12J","H12K","H12L","H12M","H12N","H12O","H12P","H12Q","H12R","H12S","H12T","H12U","H12V","H12W","H12X","H12Y","H12Z","H13","H13B","H14","H15","H15A","H15B","H15C","H15D","H15E","H15F","H15G","H15H","H15I","H15J","H15K","H15L","H15M","H20","H21","H21A","H31C","H31D","H31E","H32A","H32B","H32C","H32D","H32E",],axis =1)

dfx.shape
#dropped 88 features from ninth 100

#analyzing tenth 100 features
des.iloc[900:1000]

#selecting and dropping tenth 100 features
dfx = dfx.drop(["H32F","H32G","H32H","H32I","H32J","H32K","H32L","H32M","H32N","H32O","H32P","H32Q","H32R","H32S","H32T","H32U","H32V","H32W","H32X","H32Z","H37F","H37G","H37Z","H38","H39","H44A","H44B","H44C","H45","H46A","H46B","H47","HW15","HW17","HW18","HW19","HW51","HW52","HW53","HW55","HW56","HW57","HW58","ML1","ML2","ML11","ML12","ML13F"],axis =1)

dfx.shape
#dropped 48 features from tenth 100

#analyzing eleventh 100 features
des.iloc[1000:1100]

#selecting and dropping eleventh 100 features
dfx = dfx.drop(["ML13G","ML13H","ML13K","ML13P","ML13Z","ML14A","ML14B","ML14Y","ML14Z","ML15A","ML15B","ML15C","ML16A","ML16B","ML16C","ML17A","ML17B","ML17C","ML18A","ML18B","ML18C","ML19A","ML19B","ML19C","ML19D","ML19E","ML19F","ML19X","ML19Y","ML19Z","ML20A","ML20B","ML20C","ML21A","ML21B","ML21C","ML22A","ML22B","ML22C","ML23A","ML23B","ML23C","ML24C","ML25A","S109","S121A","S229B","S229CA","S229CB","S229CC","S229CD","S229CE","S229CF","S229CG","S229CX","S316","S323AA","S323AA","S323AB","S325","S816AA","S816B","S816C","S816D","S816E","S816F","S816G","S816H","S816I","S816J","S816K","S816X","S816Z","S816A","S908C","S915A","S927A","S930A","S1032A"],axis =1)

dfx.shape
#dropped 78 features from eleventh 100

#analyzing last 84 features
des.iloc[1100:]

#selecting and dropping last 84 features
dfx = dfx.drop(["S1032B","S1039A","S1107H","S1107IA","S1107IB","S1107IC","S1107ID","S1107IE","S1107IF","S1107IX","S1107IZ","S1107K","S1221A","S1228P","S1228Q","S1228R","S1228Y","S1228Z","S1228A","S1228BA","S1228BB","S1228BC","S1228BD","S1228BE","S1228BF","S1228BG","S1228BH","S1228BI","S1228BJ","S1228BK","S1228BX","S1228BY","S1228BZ","S1228CA","S1228CB","S1228CC","S1228CD","S1228CE","S1228CF","S1228CG","S1228CX","S1228CY","S1228CZ","S1229AA","S1229AB","S1229AC","S1229AD","S1229AE","S1229AF","S1229AG","S1229AH","S1229AI","S1229AJ","S1229AK","S1229AL","S1229AM","S1229AX","S1229AY","S1229AZ","S1229BA","S1229BB","S1229BC","S1229BD","S1229BE","S1229BF","S1229BX","S1229BY","S1229BZ","SD005","S437A","S443A","S451A","S469A"],axis = 1)

dfx.shape

#checking if any feature is missing the criteria
dfx.isnull().sum()

#dropping the ones missed out
dfx = dfx.drop(["V472D","H55D","H55M","H55Y","H37K","H37P","HW16",],axis =1 )

dfx.shape
#successfully dropped 865 features without losing any significant data

des.set_index('Variable', inplace=True)

#updating the variable label file 
des.set_index('Variable', inplace=True)
des = des.drop(["CASEID","BIDX","V000","V001","V002","V003","V004","V006","V007","V008","V008A","V009","V010","V011","V013","V014","V015","V016","V017","V018","V019","V019A","V020","V021","V022","V023","V026","V027","V028","V029","V030","V031","V032","V034","V040","V044","V045A","V045B","V045C","V046","V101","V102","V103","V104","V105","V105A","V107","V116","V130","V131","V133","V134","V139","V140","V141","V149","V150","V156","V166","V167","V168","V169B","V171B","V190","V190A","V202","V203","V204","V205","V208","V210","V211","V214","V215","V218","V220","V222","V223","V225","V226","V227","V229","V230","V231","V232","V233","V234","V237","V238","V239","V240","V241","V242","V243","V310","V311","V312","V315","V316","V317","V318","V319","V320","V321","V322","V323","V323A","V325A","V326","V327","V337","V359","V360","V362","V363","V362","V372","V372A","V375A","V376","V376A","V379","V380","V3A00A","V3A00B","V3A00C","V3A00D","V3A00E","V3A00F","V3A00G","V3A00H","V3A00I","V3A00J","V3A00K","V3A00L","V3A00M","V3A00N","V3A00O","V3A00P","V3A00Q","V3A00R","V3A00S","V3A00T","V3A00U","V3A00V","V3A00W","V3A00X","V3A00Y","V3A00Z","V3A01","V3A02","V3A03","V3A04","V3A05","V3A06","V3A07","V3A08A","V3A08B","V3A08C","V3A08D","V3A08E","V3A08F","V3A08G","V3A08H","V3A08I","V3A08J","V3A08K","V3A08L","V3A08M","V3A08N","V3A08O","V3A08P","V3A08Q","V3A08R","V3A08S","V3A08T","V3A08U","V3A08V","V3A08W","V3A08AA","V3A08AB","V3A08AC","V3A08AD","V3A08X","V3A08Z","V3A09A","V3A09B","V407","V408","V409A","V410A","V412","V412B","V413A","V413B","V413C","V413D","V414A","V414B","V414C","V414D","V414Q","V414R","V414S","V414T","V414U","V414V","V414W","V415","V420","V421","V452A","V452B","V452C","V453","V454","V455","V456","V457","V458","V462","V463E","V463F","V463J","V463K","V463L","V464","V465","V466","V467A","V467B","V467C","V467D","V467E","V467F","V467G","V467H","V467I","V467J","V467K","V467L","V467M","V468","V469E","V469F","V469X","V471A","V471B","V471C","V471D","V471E","V471F","V471G","V472A","V472B","V472C","V472E","V472F","V472G","V472H","V472I","V472J","V472K","V472L","V472M","V472N","V472O","V472P","V472Q","V472R","V472S","V472T","V472U","V473A","V473B","V474A","V474B","V474C","V474D","V474E","V474F","V474G","V474H","V474I","V474J","V474X","V474Z","V475","V476","V477","V478","V479","V480","V481","V481A","V481B","V481C","V481D","V481E","V481F","V481G","V481H","V481X","V482A","V482B","V482C","V501","V502","V503","V504","V505","V506","V507","V508","V509","V510","V512","V513","V527","V528","V529","V530","V531","V532","V535","V537","V538","V539","V540","V541","V603","V604","V613","V616","V626","V625A","V626A","V627","V628","V629","V631","V632","V633A","V633B","V633C","V633D","V633E","V633F","V633G","V634","V702","V704","V704A","V714A","V715","V716","V719","V721","V729","V731","V732","V739","V740","V741","V743A","V743B","V743C","V743D","V743E","V743F","V744A","V744B","V744C","V744D","V744E","V745A","V745B","V745C","V745D","V746","B1","B2","B3","B6","B7","B8","B9","B10","B12","B13","B17","B18","B19","B20","M1B","M1C","M1D","M1E","M2C","M2D","M2E","M2F","M2M","M3F","M2K","M2I","M2J","M2M","M3H","M3J","M3K","M3L","M3N","M11","M17A","M19A","M27","M28","M29","M35","M36","M42A","M42B","M43","M44","M47","M48","M49A","M49B","M49C","M49D","M49E","M49F","M49G","M49X","M49Z","M49Y","M54","M55A","M55B","M55C","M55D","M55E","M55F","M55G","M55H","M55I","M55J","M55K","M55L","M55M","M55N","M55O","M55X","M55Z","M57C","M57D","M57I","M57J","M57K","M57L","M57Q","M57R","M57S","M57T","M57U","M57V","M57X","M61","M63","M65A","M65B","M65C","M65D","M65E","M65F","M65G","M65H","M65I","M65J","M65K","M65L","M65X","M66","M67","M68","M69","M71","M72","M73","M75","M76","M78F","M78G","M78H","M78I","M78J","HIDX","H1","H1A","H2D","H2M","H2Y","H3D","H3M","H3Y","H4D","H4M","H4Y","H5D","H5Y","H5M","H6D","H6M","H6Y","H7D","H7M","H7Y","H8D","H8M","H8Y","H9D","H9M","H9Y","H9AD","H9AM","H9AY","H0D","H0M","H0Y","H33D","H33M","H33Y","H35","H36A","H36B","H36C","H36D","H36E","H36F","H40","H40D","H40M","H40Y","H41A","H41B","H50","H50D","H50M","H50Y","H51","H51D","H51M","H51Y","H52D","H52M","H52Y","H53D","H53M","H53Y","H54D","H54M","H54Y","H56D","H56M","H56Y","H57","H57D","H57M","H57Y","H58","H58D","H58M","H58Y","H59","H59D","H59M","H59Y","H60D","H60M","H60Y","H61D","H61M","H61Y","H62D","H62M","H62Y","H63D","H63M","H63Y","H64D","H64M","H64Y","H65D","H65M","H65Y","H66D","H66M","H66Y","H80A","H80B","H80C","H80D","H80E","H80F","H80G","H11B","H12A","H12B","H12C","H12D","H12E","H12F","H12G","H12H","H12I","H12J","H12K","H12L","H12M","H12N","H12O","H12P","H12Q","H12R","H12S","H12T","H12U","H12V","H12W","H12X","H12Y","H12Z","H13","H13B","H14","H15","H15A","H15B","H15C","H15D","H15E","H15F","H15G","H15H","H15I","H15J","H15K","H15L","H15M","H20","H21","H21A","H31C","H31D","H31E","H32A","H32B","H32C","H32D","H32E","H32F","H32G","H32H","H32I","H32J","H32K","H32L","H32M","H32N","H32O","H32P","H32Q","H32R","H32S","H32T","H32U","H32V","H32W","H32X","H32Z","H37F","H37G","H37Z","H38","H39","H44A","H44B","H44C","H45","H46A","H46B","H47","HW15","HW17","HW18","HW19","HW51","HW52","HW53","HW55","HW56","HW57","HW58","ML1","ML2","ML11","ML12","ML13F","ML13G","ML13H","ML13K","ML13P","ML13Z","ML14A","ML14B","ML14Y","ML14Z","ML15A","ML15B","ML15C","ML16A","ML16B","ML16C","ML17A","ML17B","ML17C","ML18A","ML18B","ML18C","ML19A","ML19B","ML19C","ML19D","ML19E","ML19F","ML19X","ML19Y","ML19Z","ML20A","ML20B","ML20C","ML21A","ML21B","ML21C","ML22A","ML22B","ML22C","ML23A","ML23B","ML23C","ML24C","ML25A","S109","S121A","S229B","S229CA","S229CB","S229CC","S229CD","S229CE","S229CF","S229CG","S229CX","S316","S323AA","S323AA","S323AB","S325","S816AA","S816B","S816C","S816D","S816E","S816F","S816G","S816H","S816I","S816J","S816K","S816X","S816Z","S816A","S908C","S915A","S927A","S930A","S1032A","S1032B","S1039A","S1107H","S1107IA","S1107IB","S1107IC","S1107ID","S1107IE","S1107IF","S1107IX","S1107IZ","S1107K","S1221A","S1228P","S1228Q","S1228R","S1228Y","S1228Z","S1228A","S1228BA","S1228BB","S1228BC","S1228BD","S1228BE","S1228BF","S1228BG","S1228BH","S1228BI","S1228BJ","S1228BK","S1228BX","S1228BY","S1228BZ","S1228CA","S1228CB","S1228CC","S1228CD","S1228CE","S1228CF","S1228CG","S1228CX","S1228CY","S1228CZ","S1229AA","S1229AB","S1229AC","S1229AD","S1229AE","S1229AF","S1229AG","S1229AH","S1229AI","S1229AJ","S1229AK","S1229AL","S1229AM","S1229AX","S1229AY","S1229AZ","S1229BA","S1229BB","S1229BC","S1229BD","S1229BE","S1229BF","S1229BX","S1229BY","S1229BZ","SD005","S437A","S443A","S451A","S469A","V472D","H55D","H55M","H55Y","H37K","H37P","HW16"],axis = 0)
des.shape

des.shape

des.to_csv("name_labels_updated_1.csv")
dfx.to_csv('infants_updated_1.csv')

"""## iii) Correcting the data types

### Before any further analysis  I need to make sure that my datatypes are correctly defined.This will help  my model to learn smoothly
"""

#uploading the updated files
dfx = pd.read_csv("infants_updated_1.csv",low_memory = False)
dfx = dfx.drop(["Unnamed: 0"],axis =1)
des = pd.read_csv("name_labels_updated_1.csv")
des.set_index('Variable', inplace=True)

des.iloc[:100]

print(dfx.dtypes)
c = dfx.dtypes == "object"
d = dfx.dtypes == "float64"
print(c.sum()) #282
print(d.sum()) #38

dfx.head()

des.head()

print(des.shape)
print(dfx.shape)

dfx.info()

print("These are numeric features in the data with descriptions:",des[des['datatype'] == "float64"])

"""### What I will be doing is that I will be analyzing all object data type attributes and check whether numeric attributes are defined as object.But note that I will be replacing "not a de jure resident" as missing because this is what survey speicialists has reported."""

print("These are categorical features in the data with descriptions:",des[des['datatype'] == "object "])

# In feature V115 replace Not a dejure resident and Don't know with nan and On premises water avaibilty means distance covered is 0  
dfx.replace( {'V115': {'Not a dejure resident': np.nan}},inplace = True)
dfx.replace( {'V115': {"Don't know": np.nan}},inplace = True)
dfx.replace( {'V115': {"On premises": 0}},inplace = True)
#now changing dtpe to float64
dfx["V115"] = dfx['V115'].astype('float64')

#change dtype of V152--- age of household to float64
dfx.replace( {'V152': {'95+': 95}},inplace = True)
dfx["V152"] = dfx['V152'].astype('float64')

#change dtype of V235 ---Index last child prior to maternity-health (calendar)to float 64
dfx.replace( {'V235': {'No prior child': 0}},inplace = True)
dfx["V152"] = dfx['V152'].astype('float64')

#change dtype of V426 ---when child put to breast to float 64 in number of hours
dfx.replace( {'V426': {'Immediately': 0}},inplace = True)
dfx.replace( {'V426': {'Within first hour': 0}},inplace = True)
dfx.replace( {'V426': {'Hours: 1': 1}},inplace = True)
dfx.replace( {'V426': {'101.0': 1}},inplace = True)
dfx.replace( {'V426': {'102.0': 2}},inplace = True)
dfx.replace( {'V426': {'103.0': 3}},inplace = True)
dfx.replace( {'V426': {'104.0': 4}},inplace = True)
dfx.replace( {'V426': {'105.0': 5}},inplace = True)
dfx.replace( {'V426': {'106.0': 6}},inplace = True)
dfx.replace( {'V426': {'107.0': 7}},inplace = True)
dfx.replace( {'V426': {'108.0': 8}},inplace = True)
dfx.replace( {'V426': {'110.0': 10}},inplace = True)
dfx.replace( {'V426': {'112.0': 12}},inplace = True)
dfx.replace( {'V426': {'109.0': 9}},inplace = True)
dfx.replace( {'V426': {'114.0': 14}},inplace = True)
dfx.replace( {'V426': {'115.0': 15}},inplace = True)
dfx.replace( {'V426': {'113.0': 13}},inplace = True)
dfx.replace( {'V426': {'116.0': 16}},inplace = True)
dfx.replace( {'V426': {'111.0': 11}},inplace = True)
dfx.replace( {'V426': {'120.0': 20}},inplace = True)
dfx.replace( {'V426': {'201.0': 24}},inplace = True)
dfx.replace( {'V426': {'202.0': 48}},inplace = True)
dfx.replace( {'V426': {'203.0': 72}},inplace = True)
dfx.replace( {'V426': {'204.0': 96}},inplace = True)
dfx.replace( {'V426': {'205.0': 120}},inplace = True)
dfx.replace( {'V426': {'206.0': 144}},inplace = True)
dfx.replace( {'V426': {'207.0': 168}},inplace = True)
dfx.replace( {'V426': {'208.0': 192}},inplace = True)
dfx.replace( {'V426': {'209.0': 216}},inplace = True)
dfx.replace( {'V426': {'216.0': 384}},inplace = True)
dfx.replace( {'V426': {'211.0': 264}},inplace = True)
dfx.replace( {'V426': {'214.0': 336}},inplace = True)
dfx.replace( {'V426': {'215.0': 336}},inplace = True)
dfx.replace( {'V426': {'220.0': 480}},inplace = True)
dfx.replace( {'V426': {'221.0': 504}},inplace = True)
dfx.replace( {'V426': {'230.0': 720}},inplace = True)
dfx.replace( {'V426': {'199': np.nan}},inplace = True)
dfx.replace( {'V426': {'299': np.nan}},inplace = True)
dfx.replace( {'V426': {'Days: 1': 24}},inplace = True)
dfx["V152"] = dfx['V152'].astype('float64')

#change dtype of V437---Respondent's weight in kilograms (1 decimal)  to float64 
dfx.replace( {'V437': {'Refused': np.nan}},inplace = True)
dfx.replace( {'V437': {'Not present': np.nan}},inplace = True)
dfx.replace( {'V437': {'Other': np.nan}},inplace = True)
dfx["V437"] = dfx['V437'].astype('float64')

#change dtype of V438---Respondent's height in cms(1 decimal)  to float64
dfx.replace( {'V438': {'Refused': np.nan}},inplace = True)
dfx.replace( {'V438': {'Not present': np.nan}},inplace = True)
dfx.replace( {'V438': {'Other': np.nan}},inplace = True)
dfx["V438"] = dfx['V438'].astype('float64')

#change dtype of V439---Height/Age percentile to float64
#Flagged cases
dfx.replace( {'V439': {'Flagged cases': np.nan}},inplace = True)
dfx["V439"] = dfx['V439'].astype('float64')

#change dtype of V440
dfx.replace( {'V440': {'Flagged cases': np.nan}},inplace = True)
dfx["V440"] = dfx['V440'].astype('float64')

#change dtype of V441
dfx.replace( {'V441': {'Flagged cases': np.nan}},inplace = True)
dfx["V441"] = dfx['V441'].astype('float64')

#change dtype of V442
dfx.replace( {'V442': {'Flagged cases': np.nan}},inplace = True)
dfx["V442"] = dfx['V442'].astype('float64')

#change dtype of V443
dfx.replace( {'V443': {'Flagged cases': np.nan}},inplace = True)
dfx["V443"] = dfx['V443'].astype('float64')

#change dtype of V444
dfx.replace( {'V444': {'Flagged cases': np.nan}},inplace = True)
dfx["V444"] = dfx['V444'].astype('float64')

#change dtype of V444A
dfx.replace( {'V444A': {'Flagged cases': np.nan}},inplace = True)
dfx["V444A"] = dfx['V444A'].astype('float64')

#change dtype of V445
dfx.replace( {'V445': {'Flagged cases': np.nan}},inplace = True)
dfx["V445"] = dfx['V445'].astype('float64')

#change dtype of V446
dfx.replace( {'V446': {'Flagged cases': np.nan}},inplace = True)
dfx["V446"] = dfx['V446'].astype('float64')

#change dtype of V525
dfx.replace( {'V525': {"Don't know": np.nan}},inplace = True)
dfx["V525"] = dfx['V525'].astype('float64')

#change dtype of V614
dfx.replace( {'V614': {'6+': 6}},inplace = True)
dfx.replace( {'V614': {"Non-numeric response": np.nan}},inplace = True)
dfx["V614"] = dfx['V614'].astype('float64')

#change dtype of B16
dfx.replace( {'B16': {"Not listed in household": np.nan}},inplace = True)
dfx["B16"] = dfx['B16'].astype('float64')

#change dtype of M1
dfx.replace( {'M1': {'7+': 7}},inplace = True)
dfx.replace( {'M1': {"Received no injection": 0}},inplace = True)
dfx.replace( {'M1': {"Don't know": np.nan}},inplace = True)
dfx["M1"] = dfx['M1'].astype('float64')

#change dtype of M1A
dfx.replace( {'M1A': {'7+': 7}},inplace = True)
dfx.replace( {'M1A': {"Received no injection": 0}},inplace = True)
dfx.replace( {'M1A': {"Don't know": np.nan}},inplace = True)
dfx["M1A"] = dfx['M1A'].astype('float64')

#change dtype of M5
dfx.replace( {'M5': {"Ever breastfed, not currently breastfeeding": 0}},inplace = True)
dfx.replace( {'M5': {"Never breastfed": 0}},inplace = True)
dfx["M5"] = dfx['M5'].astype('float64')

#change dtype of M6
dfx.replace( {'M6': {"Period not returned": 0}},inplace = True)
dfx.replace( {'M6': {"Don't know": np.nan}},inplace = True)
dfx["M6"] = dfx['M6'].astype('float64')

#change dtype of M7
dfx.replace( {'M7': {"Don't know": np.nan}},inplace = True)
dfx.replace( {'M7': {"Inconsistent": np.nan}},inplace = True)
dfx["M7"] = dfx['M7'].astype('float64')

#change dtype of M8
dfx.replace( {'M8': {"Still abstaining": 1}},inplace = True)
dfx.replace( {'M8': {"Don't know": np.nan}},inplace = True)
dfx["M8"] = dfx['M8'].astype('float64')

#change dtype of M9
dfx.replace( {'M9': {"Don't know": np.nan}},inplace = True)
dfx.replace( {'M9': {"Inconsistent": np.nan}},inplace = True)
dfx["M9"] = dfx['M9'].astype('float64')

#change dtype of M13
dfx.replace( {'M13': {"Don't know": np.nan}},inplace = True)
dfx["M13"] = dfx['M13'].astype('float64')

#change dtype of M14
dfx.replace( {'M14': {"No antenatal visits": 0}},inplace = True)
dfx.replace( {'M14': {"Don't know": np.nan}},inplace = True)
dfx["M14"] = dfx['M14'].astype('float64')

#change dtype of M19
dfx.replace( {'M19': {"Not weighed at birth": np.nan}},inplace = True)
dfx.replace( {'M19': {"Don't know": np.nan}},inplace = True)
dfx["M19"] = dfx['M19'].astype('float64')

#drop M34---when child put to breast because this feature is already present at  V426 and it repeats
dfx = dfx.drop(["M34"],axis =1)

#change dtype of M39
dfx.replace( {'M39': {"None": 0}},inplace = True)
dfx.replace( {'M39': {"Don't know": np.nan}},inplace = True)
dfx.replace( {'M39': {'7+': 7}},inplace = True)
dfx["M39"] = dfx['M39'].astype('float64')

#change dtype of M46
dfx.replace( {'M46': {"Were not taken": 0}},inplace = True)
dfx.replace( {'M46': {"Don't know": np.nan}},inplace = True)
dfx["M46"] = dfx['M46'].astype('float64')

#change dtype of HW2
dfx.replace( {'HW2': {"Refused": np.nan}},inplace = True)
dfx.replace( {'HW2': {"Not present": np.nan}},inplace = True)
dfx.replace( {'HW2': {"Other": np.nan}},inplace = True)
dfx["HW2"] = dfx['HW2'].astype('float64')

#change dtype of HW3
dfx.replace( {'HW3': {"Refused": np.nan}},inplace = True)
dfx.replace( {'HW3': {"Not present": np.nan}},inplace = True)
dfx.replace( {'HW3': {"Other": np.nan}},inplace = True)
dfx["HW3"] = dfx['HW3'].astype('float64')

#change dtype of HW4
dfx.replace( {'HW4': {"Flagged cases": np.nan}},inplace = True)
dfx.replace( {'HW4': {"9980.0": np.nan}},inplace = True)
dfx["HW4"] = dfx['HW4'].astype('float64')

#change dtype of HW5
dfx.replace( {'HW5': {"Flagged cases": np.nan}},inplace = True)
dfx.replace( {'HW5': {"9980.0": np.nan}},inplace = True)
dfx["HW5"] = dfx['HW5'].astype('float64')

#change dtype of HW6
dfx.replace( {'HW6': {"Flagged cases": np.nan}},inplace = True)
dfx.replace( {'HW6': {"9980.0": np.nan}},inplace = True)
dfx["HW6"] = dfx['HW6'].astype('float64')

#change dtype of HW7
dfx.replace( {'HW7': {"Flagged cases": np.nan}},inplace = True)
dfx.replace( {'HW7': {"9980.0": np.nan}},inplace = True)
dfx["HW7"] = dfx['HW7'].astype('float64')

#change dtype of HW8
dfx.replace( {'HW8': {"Flagged cases": np.nan}},inplace = True)
dfx.replace( {'HW8': {"9980.0": np.nan}},inplace = True)
dfx["HW8"] = dfx['HW8'].astype('float64')

#change dtype of HW9
dfx.replace( {'HW9': {"Flagged cases": np.nan}},inplace = True)
dfx.replace( {'HW9': {"9980.0": np.nan}},inplace = True)
dfx["HW9"] = dfx['HW9'].astype('float64')

#change dtype of HW10
dfx.replace( {'HW10': {"Flagged cases": np.nan}},inplace = True)
dfx.replace( {'HW10': {"9980.0": np.nan}},inplace = True)
dfx["HW10"] = dfx['HW10'].astype('float64')

#change dtype of HW11
dfx.replace( {'HW11': {"Flagged cases": np.nan}},inplace = True)
dfx.replace( {'HW11': {"9980.0": np.nan}},inplace = True)
dfx["HW11"] = dfx['HW11'].astype('float64')

#change dtype of HW12
dfx.replace( {'HW12': {"Flagged cases": np.nan}},inplace = True)
dfx.replace( {'HW12': {"9980.0": np.nan}},inplace = True)
dfx["HW12"] = dfx['HW12'].astype('float64')

#change dtype of HW70
dfx.replace( {'HW70': {"Flagged cases": np.nan}},inplace = True)
dfx.replace( {'HW70': {"9980.0": np.nan}},inplace = True)
dfx["HW70"] = dfx['HW70'].astype('float64')

#change dtype of HW71
dfx.replace( {'HW71': {"Flagged cases": np.nan}},inplace = True)
dfx.replace( {'HW71': {"9980.0": np.nan}},inplace = True)
dfx["HW71"] = dfx['HW71'].astype('float64')

#change dtype of HW72
dfx.replace( {'HW72': {"Flagged cases": np.nan}},inplace = True)
dfx.replace( {'HW72': {"Height out of plausible limits": np.nan}},inplace = True)
dfx["HW72"] = dfx['HW72'].astype('float64')

#change dtype of HW73
dfx.replace( {'HW73': {"Flagged cases": np.nan}},inplace = True)
dfx["HW73"] = dfx['HW73'].astype('float64')

#change dtype of S1107E
dfx.replace( {'S1107E': {"7 or more months": 7}},inplace = True)
dfx.replace( {'S1107E': {"Don't know": np.nan}},inplace = True)
dfx["S1107E"] = dfx['S1107E'].astype('float64')

print(dfx.dtypes)
c = dfx.dtypes == "object"
d = dfx.dtypes == "float64"
print(c.sum()) #236
print(d.sum()) #83

des.head()

#updating the variable file
rows = ["V115",'V152',"V235","V426","V437","V438","V439","V440","V441","V442","V443","V444","V444A","V445","V446","V525","V614","B16","M1","M1A","M5","M6","M7","M8","M9","M13","M14","M19","M39","M46","HW2","HW3","HW4","HW5","HW6","HW7","HW8","HW9","HW10","HW11","HW12","HW70","HW71","HW72","HW73","S1107E",]
for v in rows:
    des["datatype"][v] ="float64"

des.datatype

"""### Looks like  I have successfully converted those attributes should be numeric datatype.Now I will be analyzing the object attributes again and will try to excess boolean datatypes."""

print("These are categorical features in the data with descriptions:",des[des['datatype'] == "object "])

#V213 updating
dfx.replace( {'V213': {"No or unsure": "No"}},inplace = True)

#V244 updating
dfx.replace( {'V244': {"Don't know": np.nan}},inplace = True)

#V367 updating
dfx.replace( {'V367': {"Wanted then": "Yes"}},inplace = True)
dfx.replace( {'V367': {"Wanted later": "Yes"}},inplace = True)
dfx.replace( {'V367': {"Wanted no more": "No"}},inplace = True)

#V416 updating
dfx.replace( {'V416': {"Heard of ORS": "Yes"}},inplace = True)
dfx.replace( {'V416': {"Used ORS": "Yes"}},inplace = True)
dfx.replace( {'V416': {"Never heard of": "No"}},inplace = True)

#V447 updating
dfx.replace( {'V447': {"No measurement found in household": "No"}},inplace = True)
dfx.replace( {'V447': {"Measured": "Yes"}},inplace = True)
dfx.replace( {'V447': {"Some children": "No"}},inplace = True)
dfx.replace( {'V447': {"Other": "No"}},inplace = True)
dfx.replace( {'V447': {"Not present": "No"}},inplace = True)

#V460 updating
dfx.replace( {'V460': {"No net in household": "No"}},inplace = True)
dfx.replace( {'V460': {"All children": "Yes"}},inplace = True)
dfx.replace( {'V460': {"Some children": "Yes"}},inplace = True)

#dropping V463AA and V463AB because it repeats information on V463A and dropping V463X and V463D because all values are No
#and updating V463Z
dfx = dfx.drop(["V463AA","V463AB","V463X","V463D"],axis = 1)
dfx.replace( {'V463Z': {"Yes, smokes nothing": "Yes"}},inplace = True)

#updating V536
dfx.replace( {'V536': {"Active in last 4 weeks": "Yes"}},inplace = True)
dfx.replace( {'V536': {"Not active in last 4 weeks - postpartum abstinence": "No"}},inplace = True)
dfx.replace( {'V536': {"Not active in last 4 weeks - not postpartum abstinence": "No"}},inplace = True)

#updating V602
dfx.replace( {'V602': {"Have another": "Yes"}},inplace = True)
dfx.replace( {'V602': {"No more": "No"}},inplace = True)
dfx.replace( {'V602': {"Sterilized (respondent or partner)": "No"}},inplace = True)
dfx.replace( {'V602': {"Declared infecund": "No"}},inplace = True)
dfx.replace( {'V602': {"Undecided": "No"}},inplace = True)

#updating V605
dfx.replace( {'V605': {"Wants after 2+ years": "Yes"}},inplace = True)
dfx.replace( {'V605': {"Wants within 2 years": "Yes"}},inplace = True)
dfx.replace( {'V605': {"Wants, unsure timing": "Yes"}},inplace = True)
dfx.replace( {'V605': {"Sterilized (respondent or partner)": "No"}},inplace = True)
dfx.replace( {'V605': {"Declared infecund": "No"}},inplace = True)
dfx.replace( {'V605': {"Wants no more": "No"}},inplace = True)
dfx.replace( {'V605': {"Undecided": "No"}},inplace = True)

#updating V621
dfx.replace( {'V621': {"Both want same": "Yes"}},inplace = True)
dfx.replace( {'V621': {"Husband wants more": "Yes"}},inplace = True)
dfx.replace( {'V621': {"Husband wants fewer": "Yes"}},inplace = True)
dfx.replace( {'V621': {"Don't know": "No"}},inplace = True)

#updating B0
dfx.replace( {'B0': {"Single birth": "No"}},inplace = True)
dfx.replace( {'B0': {"1st of multiple": "Yes"}},inplace = True)
dfx.replace( {'B0': {"2nd of multiple": "Yes"}},inplace = True)
dfx.replace( {'B0': {"3rd of multiple": "Yes"}},inplace = True)

#updating M2N
dfx.replace( {'M2N': {"No: some care": "Yes"}},inplace = True)
dfx.replace( {'M2N': {"Yes: no care": "No"}},inplace = True)

#updating M4
dfx.replace( {'M4': {"Never breastfed": "No"}},inplace = True)
dfx.replace( {'M4': {"Ever breastfed, not currently breastfeeding": "Yes"}},inplace = True)
dfx.replace( {'M4': {"Still breastfeeding": "Yes"}},inplace = True)

#updating M10
dfx.replace( {'M10': {"No more": "No"}},inplace = True)
dfx.replace( {'M10': {"Later": "Yes"}},inplace = True)
dfx.replace( {'M10': {"Then": "Yes"}},inplace = True)

#updating M38
dfx.replace( {'M38': {"Don't know": np.nan}},inplace = True)

#updating M39A
dfx.replace( {'M39A': {"Don't know": np.nan}},inplace = True)

#updating M45
dfx.replace( {'M45': {"Don't know": np.nan}},inplace = True)

#updating M60
dfx.replace( {'M60': {"Don't know": np.nan}},inplace = True)

#updating M70
dfx.replace( {'M70': {"Don't know": np.nan}},inplace = True)

#updating M74
dfx.replace( {'M74': {"Don't know": np.nan}},inplace = True)

#updating M77
dfx.replace( {'M77': {"Put on chest, no touching of bare skin": "No"}},inplace = True)
dfx.replace( {'M77': {"Put on chest, touching bare skin": "Yes"}},inplace = True)
dfx.replace( {'M77': {"Don't know": np.nan}},inplace = True)
dfx.replace( {'M77': {"Put on chest, DK/Missing on touching of bare skin": "Yes"}},inplace = True)

#updating M78A
dfx.replace( {'M78A': {"Don't know": np.nan}},inplace = True)

#updating M78B
dfx.replace( {'M78B': {"Don't know": np.nan}},inplace = True)

#updating M78C
dfx.replace( {'M78C': {"Don't know": np.nan}},inplace = True)

#updating M78D
dfx.replace( {'M78D': {"Don't know": np.nan}},inplace = True)

#updating M78E
dfx.replace( {'M78E': {"Don't know": np.nan}},inplace = True)

#updating H2
dfx.replace( {'H2': {"Don't know": np.nan}},inplace = True)
dfx.replace( {'H2': {"Vaccination date on card": "Yes"}},inplace = True)
dfx.replace( {'H2': {"Reported by mother": "Yes"}},inplace = True)
dfx.replace( {'H2': {"Vaccination marked on card": "Yes"}},inplace = True)

#updating H3
dfx.replace( {'H3': {"Don't know": np.nan}},inplace = True)
dfx.replace( {'H3': {"Vaccination date on card": "Yes"}},inplace = True)
dfx.replace( {'H3': {"Reported by mother": "Yes"}},inplace = True)
dfx.replace( {'H3': {"Vaccination marked on card": "Yes"}},inplace = True)

#updating H4
dfx.replace( {'H4': {"Don't know": np.nan}},inplace = True)
dfx.replace( {'H4': {"Vaccination date on card": "Yes"}},inplace = True)
dfx.replace( {'H4': {"Reported by mother": "Yes"}},inplace = True)
dfx.replace( {'H4': {"Vaccination marked on card": "Yes"}},inplace = True)

#updating H5
dfx.replace( {'H5': {"Don't know": np.nan}},inplace = True)
dfx.replace( {'H5': {"Vaccination date on card": "Yes"}},inplace = True)
dfx.replace( {'H5': {"Reported by mother": "Yes"}},inplace = True)
dfx.replace( {'H5': {"Vaccination marked on card": "Yes"}},inplace = True)

#updating H6
dfx.replace( {'H6': {"Don't know": np.nan}},inplace = True)
dfx.replace( {'H6': {"Vaccination date on card": "Yes"}},inplace = True)
dfx.replace( {'H6': {"Reported by mother": "Yes"}},inplace = True)
dfx.replace( {'H6': {"Vaccination marked on card": "Yes"}},inplace = True)

#updating H7
dfx.replace( {'H7': {"Don't know": np.nan}},inplace = True)
dfx.replace( {'H7': {"Vaccination date on card": "Yes"}},inplace = True)
dfx.replace( {'H7': {"Reported by mother": "Yes"}},inplace = True)
dfx.replace( {'H7': {"Vaccination marked on card": "Yes"}},inplace = True)

#updating H8
dfx.replace( {'H8': {"Don't know": np.nan}},inplace = True)
dfx.replace( {'H8': {"Vaccination date on card": "Yes"}},inplace = True)
dfx.replace( {'H8': {"Reported by mother": "Yes"}},inplace = True)
dfx.replace( {'H8': {"Vaccination marked on card": "Yes"}},inplace = True)

#updating H9
dfx.replace( {'H9': {"Don't know": np.nan}},inplace = True)
dfx.replace( {'H9': {"Vaccination date on card": "Yes"}},inplace = True)
dfx.replace( {'H9': {"Reported by mother": "Yes"}},inplace = True)
dfx.replace( {'H9': {"Vaccination marked on card": "Yes"}},inplace = True)

#updating H9A
dfx.replace( {'H9A': {"Don't know": np.nan}},inplace = True)
dfx.replace( {'H9A': {"Vaccination date on card": "Yes"}},inplace = True)
dfx.replace( {'H9A': {"Reported by mother": "Yes"}},inplace = True)
dfx.replace( {'H9A': {"Vaccination marked on card": "Yes"}},inplace = True)

#updating H0
dfx.replace( {'H0': {"Don't know": np.nan}},inplace = True)
dfx.replace( {'H0': {"Vaccination date on card": "Yes"}},inplace = True)
dfx.replace( {'H0': {"Reported by mother": "Yes"}},inplace = True)
dfx.replace( {'H0': {"Vaccination marked on card": "Yes"}},inplace = True)

#updating H10
dfx.replace( {'H10': {"Don't know": np.nan}},inplace = True)
dfx.replace( {'H10': {"Vaccination date on card": "Yes"}},inplace = True)
dfx.replace( {'H10': {"Reported by mother": "Yes"}},inplace = True)
dfx.replace( {'H10': {"Vaccination marked on card": "Yes"}},inplace = True)

#updating H33
dfx.replace( {'H33': {"Don't know": np.nan}},inplace = True)
dfx.replace( {'H33': {"Vaccination date on card": "Yes"}},inplace = True)
dfx.replace( {'H33': {"Reported by mother": "Yes"}},inplace = True)
dfx.replace( {'H33': {"Vaccination marked on card": "Yes"}},inplace = True)

#updating H52
dfx.replace( {'H52': {"Don't know": np.nan}},inplace = True)
dfx.replace( {'H52': {"Vaccination date on card": "Yes"}},inplace = True)
dfx.replace( {'H52': {"Reported by mother": "Yes"}},inplace = True)
dfx.replace( {'H52': {"Vaccination marked on card": "Yes"}},inplace = True)

#updating H53
dfx.replace( {'H53': {"Don't know": np.nan}},inplace = True)
dfx.replace( {'H53': {"Vaccination date on card": "Yes"}},inplace = True)
dfx.replace( {'H53': {"Reported by mother": "Yes"}},inplace = True)
dfx.replace( {'H53': {"Vaccination marked on card": "Yes"}},inplace = True)

#updating H54
dfx.replace( {'H54': {"Don't know": np.nan}},inplace = True)
dfx.replace( {'H54': {"Vaccination date on card": "Yes"}},inplace = True)
dfx.replace( {'H54': {"Reported by mother": "Yes"}},inplace = True)
dfx.replace( {'H54': {"Vaccination marked on card": "Yes"}},inplace = True)

#updating H55
dfx.replace( {'H55': {"Don't know": np.nan}},inplace = True)
dfx.replace( {'H55': {"Vaccination date on card": "Yes"}},inplace = True)
dfx.replace( {'H55': {"Reported by mother": "Yes"}},inplace = True)
dfx.replace( {'H55': {"Vaccination marked on card": "Yes"}},inplace = True)

#updating H56
dfx.replace( {'H56': {"Don't know": np.nan}},inplace = True)
dfx.replace( {'H56': {"Vaccination date on card": "Yes"}},inplace = True)
dfx.replace( {'H56': {"Reported by mother": "Yes"}},inplace = True)
dfx.replace( {'H56': {"Vaccination marked on card": "Yes"}},inplace = True)

#updating H60
dfx.replace( {'H60': {"Don't know": np.nan}},inplace = True)
dfx.replace( {'H60': {"Vaccination date on card": "Yes"}},inplace = True)
dfx.replace( {'H60': {"Reported by mother": "Yes"}},inplace = True)
dfx.replace( {'H60': {"Vaccination marked on card": "Yes"}},inplace = True)

#updating H61
dfx.replace( {'H61': {"Don't know": np.nan}},inplace = True)
dfx.replace( {'H61': {"Vaccination date on card": "Yes"}},inplace = True)
dfx.replace( {'H61': {"Reported by mother": "Yes"}},inplace = True)
dfx.replace( {'H61': {"Vaccination marked on card": "Yes"}},inplace = True)

#updating H62
dfx.replace( {'H62': {"Don't know": np.nan}},inplace = True)
dfx.replace( {'H62': {"Vaccination date on card": "Yes"}},inplace = True)
dfx.replace( {'H62': {"Reported by mother": "Yes"}},inplace = True)
dfx.replace( {'H62': {"Vaccination marked on card": "Yes"}},inplace = True)

#updating H63
dfx.replace( {'H63': {"Don't know": np.nan}},inplace = True)
dfx.replace( {'H63': {"Vaccination date on card": "Yes"}},inplace = True)
dfx.replace( {'H63': {"Reported by mother": "Yes"}},inplace = True)
dfx.replace( {'H63': {"Vaccination marked on card": "Yes"}},inplace = True)

#updating H64
dfx.replace( {'H64': {"Don't know": np.nan}},inplace = True)
dfx.replace( {'H64': {"Vaccination date on card": "Yes"}},inplace = True)
dfx.replace( {'H64': {"Reported by mother": "Yes"}},inplace = True)
dfx.replace( {'H64': {"Vaccination marked on card": "Yes"}},inplace = True)

#updating H65
dfx.replace( {'H65': {"Don't know": np.nan}},inplace = True)
dfx.replace( {'H65': {"Vaccination date on card": "Yes"}},inplace = True)
dfx.replace( {'H65': {"Reported by mother": "Yes"}},inplace = True)
dfx.replace( {'H65': {"Vaccination marked on card": "Yes"}},inplace = True)

#updating H66
dfx.replace( {'H66': {"Don't know": np.nan}},inplace = True)
dfx.replace( {'H66': {"Vaccination date on card": "Yes"}},inplace = True)
dfx.replace( {'H66': {"Reported by mother": "Yes"}},inplace = True)
dfx.replace( {'H66': {"Vaccination marked on card": "Yes"}},inplace = True)

#updating H11
dfx.replace( {'H11': {"Don't know": np.nan}},inplace = True)
dfx.replace( {'H11': {"Yes, last two weeks": "Yes"}},inplace = True)

#updating H22
dfx.replace( {'H22': {"Don't know": np.nan}},inplace = True)

#updating H31
dfx.replace( {'H31': {"Don't know": np.nan}},inplace = True)
dfx.replace( {'H31': {"Yes, last two weeks": "Yes"}},inplace = True)

#updating H31B
dfx.replace( {'H31B': {"Don't know": np.nan}},inplace = True)

#updating H32Y
dfx.replace( {'H32Y': {"No: received treatment": "No"}},inplace = True)
dfx.replace( {'H32Y': {"Yes: no treatment": "Yes"}},inplace = True)

#updating H34
dfx.replace( {'H34': {"Don't know": np.nan}},inplace = True)

#removing H37H----Other antimalarial taken for fever because all values are No
dfx = dfx.drop(["H37H"],axis =1)

#updating H42
dfx.replace( {'H42': {"Don't know": np.nan}},inplace = True)

#updating H43
dfx.replace( {'H43': {"Don't know": np.nan}},inplace = True)

#updating HW13
dfx.replace( {'HW13': {"No measurement found in household": "No"}},inplace = True)
dfx.replace( {'HW13': {"Measured": "Yes"}},inplace = True)
dfx.replace( {'HW13': {"Dead": np.nan}},inplace = True)
dfx.replace( {'HW13': {"Refused": np.nan}},inplace = True)
dfx.replace( {'HW13': {"Other": np.nan}},inplace = True)
dfx.replace( {'HW13': {"Not present": np.nan}},inplace = True)

#updating ML0
dfx.replace( {'ML0': {"No net": "No"}},inplace = True)
dfx.replace( {'ML0': {"Only untreated nets": "Yes"}},inplace = True)
dfx.replace( {'ML0': {"Only treated nets": "Yes"}},inplace = True)

#dropping from ML13A to ML13Y
dfx = dfx.drop(["ML13A","ML13B","ML13C","ML13D","ML13DA","ML13E","ML13AA","ML13AB","ML13I","ML13J","ML13L","ML13M","ML13N","ML13O","ML13X","ML13Y"],axis =1)

#updating S924A
dfx.replace( {'S924A': {"Yes, house": "Yes"}},inplace = True)
dfx.replace( {'S924A': {"Yes, agricultural land": "Yes"}},inplace = True)
dfx.replace( {'S924A': {"Yes, residential plot": "Yes"}},inplace = True)
dfx.replace( {'S924A': {"Yes, non-agricultural land": "Yes"}},inplace = True)

#updating S932F
dfx.replace( {'S932F': {"Don't know": np.nan}},inplace = True)

#updating V119
dfx.replace( {'V119': {"Not a dejure resident": np.nan}},inplace = True)

#updating V120
dfx.replace( {'V120': {"Not a dejure resident": np.nan}},inplace = True)

#updating V121
dfx.replace( {'V121': {"Not a dejure resident": np.nan}},inplace = True)

#updating V122
dfx.replace( {'V122': {"Not a dejure resident": np.nan}},inplace = True)

#updating V123
dfx.replace( {'V123': {"Not a dejure resident": np.nan}},inplace = True)

#updating V124
dfx.replace( {'V124': {"Not a dejure resident": np.nan}},inplace = True)

#updating V125
dfx.replace( {'V125': {"Not a dejure resident": np.nan}},inplace = True)

#updating V153
dfx.replace( {'V153': {"Not a dejure resident": np.nan}},inplace = True)

#updating V160
dfx.replace( {'V160': {"Not a dejure resident": np.nan}},inplace = True)

#updating V414J
dfx.replace( {'V414J': {"Don't know": np.nan}},inplace = True)

#updating V447
dfx.replace( {'V447': {"Refused": np.nan}},inplace = True)

#updating H37Y
dfx.replace( {'H37Y': {"Yes, nothing taken": 1}},inplace = True)

#updating V171A
dfx.replace( {'V171A': {"Never": "No"}},inplace = True)
dfx.replace( {'V171A': {"Yes, last 12 months": "Yes"}},inplace = True)
dfx.replace( {'V171A': {"Yes, before last 12 months": "Yes"}},inplace = True)
dfx.replace( {'V171A': {"Yes, can't establish when": "Yes"}},inplace = True)

#updating ML101
dfx.replace( {'ML101': {"No net": "No"}},inplace = True)
dfx.replace( {'ML101': {"Only untreated nets": "Yes"}},inplace = True)
dfx.replace( {'ML101': {"Only treated nets": "Yes"}},inplace = True)

print("These are categorical features in the data with descriptions:",des[des['datatype'] == "object "])

bool_var = ["H43","ML101","V171A","H22","V244","V119","V120","V121","V122","V123","V124","V125","V153","V160","V169A","V170","V213","V216","V228","V367","V384A","V384B","V384C","V384D","V393","V393A","V394","V395","V401","V404","V405","V406","V409","V410","V411","V411A","V412A","V412C","V413","V414E","V414F","V414G","V414H","V414I","V414J","V414K","V414L","V414M","V414N","V414O","V414P","V416","V447","V459","V460","V461","V463A","V463Z","V463B","V463C","V463G","V463H","V463I","V474","V536","V602","V605","V621","V714","B0","B15","M2A","M2B","M2G","M2H","M2L","M2N","M3A","M3B","M3C","M3D","M3E","M3G","M3I","M3M","M4","M10","M17","M38","M39A","M42C","M42D","M42E","M45","M55","M57A","M57B","M57E","M57F","M57G","M57H","M57M","M57N","M57O","M57P","M60","M62","M70","M74","M77","M78A","M78B","M78C","M78D","M78E","H2","H3","H4","H5","H6","H7","H8","H9","H9A","H0","H10","H33","H52","H53","H54","H55","H56","H60","H61","H62","H63","H64","H65","H66","H11","H31","H31B","H32Y","H34","H37A","H37B","H37C","H37D","H37DA","H37E","H37AA","H37AB","H37I","H37J","H37L","H37M","H37N","H37O","H37X","H37Y","H42","HW13","ML0","S301A","S708A","S709A","S908A","S908B","S924A","S1107A","S932F","S1107F","S1107G","S1107J","S1110A","S409A","S413AA","S413AB","S413AC"]
result = lambda a:1 if a == "Yes" else (0 if a == "No" else a) #making all bool variables into 1 or 0
for i in bool_var:
    dfx[i] = dfx[i].map(result)
for i in bool_var:
    dfx[i] =dfx[i].astype("float64")

#updating the variable file
des = des.drop(["H37H","ML13A","ML13B","ML13C","ML13D","ML13DA","ML13E","ML13AA","ML13AB","ML13I","ML13J","ML13L","ML13M","ML13N","ML13O","ML13X","ML13Y","M34","V463AA","V463AB","V463X","V463D"])
bool_var = ["H43","ML101","V171A","H22","V244","V119","V120","V121","V122","V123","V124","V125","V153","V160","V169A","V170","V213","V216","V228","V367","V384A","V384B","V384C","V384D","V393","V393A","V394","V395","V401","V404","V405","V406","V409","V410","V411","V411A","V412A","V412C","V413","V414E","V414F","V414G","V414H","V414I","V414J","V414K","V414L","V414M","V414N","V414O","V414P","V416","V447","V459","V460","V461","V463A","V463Z","V463B","V463C","V463G","V463H","V463I","V474","V536","V602","V605","V621","V714","B0","B15","M2A","M2B","M2G","M2H","M2L","M2N","M3A","M3B","M3C","M3D","M3E","M3G","M3I","M3M","M4","M10","M17","M38","M39A","M42C","M42D","M42E","M45","M55","M57A","M57B","M57E","M57F","M57G","M57H","M57M","M57N","M57O","M57P","M60","M62","M70","M74","M77","M78A","M78B","M78C","M78D","M78E","H2","H3","H4","H5","H6","H7","H8","H9","H9A","H0","H10","H33","H52","H53","H54","H55","H56","H60","H61","H62","H63","H64","H65","H66","H11","H31","H31B","H32Y","H34","H37A","H37B","H37C","H37D","H37DA","H37E","H37AA","H37AB","H37I","H37J","H37L","H37M","H37N","H37O","H37X","H37Y","H42","HW13","ML0","S301A","S708A","S709A","S908A","S908B","S924A","S1107A","S932F","S1107F","S1107G","S1107J","S1110A","S409A","S413AA","S413AB","S413AC"]
for i in bool_var:
    des["datatype"][v] ="float64" #changed to float64 as there are missing values.

des.shape

dfx.shape

print(dfx.dtypes)
c = dfx.dtypes == "object"
d = dfx.dtypes == "float64"
print(c.sum()) #38
print(d.sum()) #260

"""### So finally I have been able to convert datatypes to numeric  and to boolean doing some feature cleaning.Now I will be changing some object data types to categorical ordinal so that I can rank them and remaining will be hot encoded

### These are the remaining object attributes: 
### 1)V024:Region 
### 2)V025:Type of place of residence	
### 3)V042:Household selected for hemoglobin
### 4)V113:Source of drinking water
### 5)V127:Main floor material
### 6)V128:Main wall material
### 7)V129:Main roof material 
### 8)V135:Usual resident or visitor
### 9) V151:Sex of household head 
### 10)V155:Literacy
### 11)V157:Frequency of reading newspaper or magazine
### 12)V158:Frequency of listening to radio	 
### 13)V159:Frequency of watching television	 
### 14)V161:Type of cooking fuel  
### 15)V217:Knowledge of ovulatory cycle 	 
### 16)V313:Current use by method type 
### 17)V361:Pattern of use 
### 18)V364:Contraceptive use and intention
### 19)V623:Exposure 
### 20)V624:Unmet need 
### 21)V625:Exposure (definition 2)  
### 22)V632A:Decision maker for not using contraception 
### 23)V705:Husband/partner's occupation (grouped) 
### 24)V717:Respondent's occupation (grouped)	 
### 25)B4:Sex of child 
### 26)M15:Place of delivery 
### 27)M18:Size of child at birth 
### 28)M64:Who checked respondent health before discharge	  
### 29)SDIST:District 
### 30)SEDUC:Woman's educational level for report
### 32)S708B:Type of relationship with husband	 
### 33)SPEDUC:Husband/partner's education level for report
"""

#dropping V024,V106 
dfx = dfx.drop(["V042","V106","V701"],axis =1)

#changing V113 values to either filtered or not filtered
dfx.replace( {'V113': {"Tube well or borehole": "Unfiltered"}},inplace = True)
dfx.replace( {'V113': {"Piped into dwelling":"Filtered" }},inplace = True)
dfx.replace( {'V113': {"Public tap/standpipe": "Unfiltered"}},inplace = True)
dfx.replace( {'V113': {"Not a dejure resident": np.nan}},inplace = True)
dfx.replace( {'V113': {"Protected spring": "Filtered"}},inplace = True)
dfx.replace( {'V113': {"Unprotected spring": "Unfiltered"}},inplace = True)
dfx.replace( {'V113': {"Protected well": "Filtered"}},inplace = True)
dfx.replace( {'V113': {"River/dam/lake/ponds/stream/canal/irrigation channel": "Unfiltered"}},inplace = True)
dfx.replace( {'V113': {"Piped to neighbor": "Filtered"}},inplace = True)
dfx.replace( {'V113': {"Piped to yard/plot": "Filtered"}},inplace = True)
dfx.replace( {'V113': {"Unprotected well": "Unfiltered"}},inplace = True)
dfx.replace( {'V113': {"Bottled water": "Filtered"}},inplace = True)
dfx.replace( {'V113': {"Filtration plant": "Filtered"}},inplace = True)
dfx.replace( {'V113': {"Tanker truck": "Filtered"}},inplace = True)
dfx.replace( {'V113': {"Cart with small tank": "Filtered"}},inplace = True)
dfx.replace( {'V113': {"Rainwater": "Unfiltered"}},inplace = True)

##changing V127 values to either Adobe or Cemented
dfx.replace( {'V127': {"Earth/sand": "Adobe"}},inplace = True)
dfx.replace( {'V127': {"Cement":"Cemented" }},inplace = True)
dfx.replace( {'V127': {"Marble": "Cemented"}},inplace = True)
dfx.replace( {'V127': {"Not a dejure resident": np.nan}},inplace = True)
dfx.replace( {'V127': {"Mats": "Adobe"}},inplace = True)
dfx.replace( {'V127': {"Chips/terrazzo": "Adobe"}},inplace = True)
dfx.replace( {'V127': {"Bricks": "Adobe"}},inplace = True)
dfx.replace( {'V127': {"Dung": "Adobe"}},inplace = True)
dfx.replace( {'V127': {"Ceramic tiles": "Cemented"}},inplace = True)
dfx.replace( {'V127': {"Other": "Adobe"}},inplace = True)
dfx.replace( {'V127': {"Wood planks": "Adobe"}},inplace = True)
dfx.replace( {'V127': {"Carpet": "Adobe"}},inplace = True)

#changing V128 values to either Adobe or Cemented
dfx.replace( {'V128': {"Bricks": "Adobe"}},inplace = True)
dfx.replace( {'V128': {"Unbaked bricks/mud":"Adobe" }},inplace = True)
dfx.replace( {'V128': {"Cement blocks": "Cemented"}},inplace = True)
dfx.replace( {'V128': {"Mud/stones": "Adobe"}},inplace = True)
dfx.replace( {'V128': {"No walls": "Adobe"}},inplace = True)
dfx.replace( {'V128': {"Not a dejure resident": np.nan}},inplace = True)
dfx.replace( {'V128': {"Stone with mud": "Adobe"}},inplace = True)
dfx.replace( {'V128': {"Bamboo/sticks/mud": "Adobe"}},inplace = True)
dfx.replace( {'V128': {"Covered adobe": "Adobe"}},inplace = True)
dfx.replace( {'V128': {"Stone with lime/cement": "Cemented"}},inplace = True)
dfx.replace( {'V128': {"Uncovered adobe": "Adobe"}},inplace = True)
dfx.replace( {'V128': {"Dirt": "Adobe"}},inplace = True)
dfx.replace( {'V128': {"Cane/palm/trunks": "Adobe"}},inplace = True)
dfx.replace( {'V128': {"Other": "Adobe"}},inplace = True)
dfx.replace( {'V128': {"Bamboo with mud": "Adobe"}},inplace = True)
dfx.replace( {'V128': {"Wood planks/shingles": "Adobe"}},inplace = True)
dfx.replace( {'V128': {"Plywood": "Adobe"}},inplace = True)
dfx.replace( {'V128': {"Reused wood": "Adobe"}},inplace = True)
dfx.replace( {'V128': {"Cement": "Cemented"}},inplace = True)

#changing V129 values to either filtered or not filtered
dfx.replace( {'V129': {"Cement/rcc": "Cemented"}},inplace = True)
dfx.replace( {'V129': {"Reinforced brick cement/rcc":"Cemented" }},inplace = True)
dfx.replace( {'V129': {"Rustic mat": "Adobe"}},inplace = True)
dfx.replace( {'V129': {"Wood": "Adobe"}},inplace = True)
dfx.replace( {'V129': {"Palm/bamboo": "Adobe"}},inplace = True)
dfx.replace( {'V129': {"Not a dejure resident": np.nan}},inplace = True)
dfx.replace( {'V129': {"Calamine/cement fiber": "Cemented"}},inplace = True)
dfx.replace( {'V129': {"Wood planks": "Adobe"}},inplace = True)
dfx.replace( {'V129': {"Other": "Adobe"}},inplace = True)
dfx.replace( {'V129': {"Metal": "Cemented"}},inplace = True)
dfx.replace( {'V129': {"Thatch/palm leaf": "Adobe"}},inplace = True)
dfx.replace( {'V129': {"Cardboard": "Adobe"}},inplace = True)
dfx.replace( {'V129': {"Sod/grass": "Adobe"}},inplace = True)
dfx.replace( {'V129': {"Ceramic tiles": "Cemented"}},inplace = True)
dfx.replace( {'V129': {"Roofing shingles": "Cemented"}},inplace = True)
dfx.replace( {'V129': {"No roof": "Adobe"}},inplace = True)
dfx.replace( {'V129': {"Asbestos": "Adobe"}},inplace = True)

#changing V161 values to either Solid or  Gaseous or Electic 
dfx.replace( {'V161': {"Wood": "Solid fuel"}},inplace = True)
dfx.replace( {'V161': {"Natural gas": "Gaseous fuel"}},inplace = True)
dfx.replace( {'V161': {"LPG": "Gaseous fuel"}},inplace = True)
dfx.replace( {'V161': {"Not a dejure resident": np.nan}},inplace = True)
dfx.replace( {'V161': {"Animal dung": "Solid fuel"}},inplace = True)
dfx.replace( {'V161': {"Charcoal": "Solid fuel"}},inplace = True)
dfx.replace( {'V161': {"Straw/shrubs/grass": "Solid fuel"}},inplace = True)
dfx.replace( {'V161': {"Agricultural crop": "Solid fuel"}},inplace = True)
dfx.replace( {'V161': {"Biogas": "Gaseous fuel"}},inplace = True)
dfx.replace( {'V161': {"Electricity": "Electric fuel"}},inplace = True)
dfx.replace( {'V161': {"Coal, lignite": "Solid fuel"}},inplace = True)

#changing V221 dtype
dfx.replace( {'V221': {"Negative interval": np.nan}},inplace = True)
dfx["V221"] = dfx['V221'].astype('float64')

#updating M18
dfx.replace( {'M18': {"Don't know": np.nan}},inplace = True)

#changing V221 dtype
dfx["V235"] = dfx['V235'].astype('float64')

#changing S708B dtype
dfx.replace( {'S708B': {"First cousin on father's side": "Cousin"}},inplace = True)
dfx.replace( {'S708B': {"First cousin on mother's side": "Cousin"}},inplace = True)
dfx.replace( {'S708B': {"Second cousin": "Cousin"}},inplace = True)
dfx.replace( {'S708B': {"Other relationship": "Other"}},inplace = True)

nominal_var = ["V024","V025","V135","V151","V161","V217","V313","V361","V364","V623","V624","V625","V632A","V705","V717","B4","M15","M64","SDIST","S708B"]

#changing data type for all nominal variable
for i in nominal_var:
    dfx[i] =dfx[i].astype("category")

# imputing of categorical features
from sklearn.impute import SimpleImputer
imp = SimpleImputer(strategy="most_frequent")

nom_frame = dfx.select_dtypes(include=['category']) 
nom_frame.shape

nom_frame[:]= imp.fit_transform(nom_frame)

nom_frame.isnull().sum()

#now applying dummy encoding
#I am dropping first to avoid multi collinearity
nom_frame = pd.get_dummies(nom_frame,drop_first=True)
nom_frame.head()

nom_frame.shape

dfx = pd.concat([dfx, nom_frame], axis=1).drop(["V024","V025","V135","V151","V161","V217","V313","V361","V364","V623","V624","V625","V632A","V705","V717","B4","M15","M64","SDIST","S708B"], axis=1)

dfx.head(2)

"""### Now working in ordinal  variable"""

#ordering V113
dfx.replace( {'V113': {"Unfiltered": 1}},inplace = True)
dfx.replace( {'V113': {"Filtered": 2}},inplace = True)

#ordering V127
dfx.replace( {'V127': {"Adobe": 1}},inplace = True)
dfx.replace( {'V127': {"Cemented": 2}},inplace = True)

#ordering V128
dfx.replace( {'V128': {"Adobe": 1}},inplace = True)
dfx.replace( {'V128': {"Cemented": 2}},inplace = True)

#ordering V129
dfx.replace( {'V129': {"Adobe": 1}},inplace = True)
dfx.replace( {'V129': {"Cemented": 2}},inplace = True)

#ordering V155
dfx.replace( {'V155': {"Cannot read at all": 1}},inplace = True)
dfx.replace( {'V155': {"Able to read only parts of sentence": 2}},inplace = True)
dfx.replace( {'V155': {"Able to read whole sentence": 3}},inplace = True)

#ordering V157
dfx.replace( {'V157': {"Not at all": 1}},inplace = True)
dfx.replace( {'V157': {"Less than once a week": 2}},inplace = True)
dfx.replace( {'V157': {"At least once a week": 3}},inplace = True)

#ordering V158
dfx.replace( {'V158': {"Not at all": 1}},inplace = True)
dfx.replace( {'V158': {"Less than once a week": 2}},inplace = True)
dfx.replace( {'V158': {"At least once a week": 3}},inplace = True)

#ordering V159
dfx.replace( {'V159': {"Not at all": 1}},inplace = True)
dfx.replace( {'V159': {"Less than once a week": 2}},inplace = True)
dfx.replace( {'V159': {"At least once a week": 3}},inplace = True)

#ordering M18
dfx.replace( {'M18': {"Very small": 1}},inplace = True)
dfx.replace( {'M18': {"Smaller than average": 1}},inplace = True)
dfx.replace( {'M18': {"Average": 1}},inplace = True)
dfx.replace( {'M18': {"Larger than average": 1}},inplace = True)
dfx.replace( {'M18': {"Very large": 1}},inplace = True)

#ordering SPEDUC
dfx.replace( {'SPEDUC': {"No education": 1}},inplace = True)
dfx.replace( {'SPEDUC': {"Primary": 2}},inplace = True)
dfx.replace( {'SPEDUC': {"Middle": 3}},inplace = True)
dfx.replace( {'SPEDUC': {"Secondary": 4}},inplace = True)
dfx.replace( {'SPEDUC': {"Higher": 5}},inplace = True)
dfx.replace( {'SPEDUC': {"Don't know": np.nan}},inplace = True)

ord_var = ["V113","V127","V128","V129","V157","V158","V159","M18","SEDUC","SPEDUC"]

#changing data type for all ordinal variable
for i in ord_var:
    dfx[i] =dfx[i].astype("category")

ord_frame = dfx.loc[:, ["V113","V127","V128","V129","V157","V158","V159","M18","SEDUC","SPEDUC"]]
ord_frame.columns = ["v113","v127","v128","v129","v157","v158","v159","m18","seduc","speduc"]

ord_frame[:]= imp.fit_transform(ord_frame)

ord_frame.isnull().sum()

dfx = pd.concat([dfx, ord_frame], axis=1).drop(["V113","V127","V128","V129","V157","V158","V159","M18","SEDUC","SPEDUC"],axis =1)

dfx.shape

dfx.isnull().sum()

dfx.head(2)

"""### now imputing all numeric attributes using sklearn iterative imputer"""

dfx.dtypes =="float64"

num_var=["V005","V012","V115","V119","V120","V121","V122","V123","V124","V125","V136","V137","V138","V152","V153","AWFACTT","AWFACTU","AWFACTR","AWFACTE","AWFACTW","V160","V169A","V170","V171A","V191","V191A","ML101","V201","V206","V207","V209","V212","V213","V216","V219","V221","V224","V228","V235","V244","V367","V384A","V384B","V384C","V384D","V393","V393A","V394","V395","V401","V404","V405","V406","V409","V410","V411","V411A","V412A","V412C","V413","V414E","V414F","V414G","V414H","V414I","V414J","V414K","V414L","V414M","V414N","V414O","V414P","V416","V417","V418","V418A","V419","V426","V437","V438","V439","V440","V441","V442","V443","V444","V444A","V445","V446","V447","V447A","V459","V460","V461","V463A","V463B","V463C","V463G","V463H","V463I","V463Z","V474","V511","V525","V536","V602","V605","V614","V621","V714","V730","BORD","B0","B11","B15","B16","MIDX","M1","M1A","M2A","M2B","M2G","M2H","M2L","M2N","M3A","M3B","M3C","M3D","M3E","M3G","M3I","M3M","M4","M5","M6","M7","M8","M9","M10","M13","M14","M17","M19","M38","M39A","M39","M42C","M42D","M42E","M45","M46","M55","M57A","M57B","M57E","M57F","M57G","M57H","M57M","M57N","M57O","M57P","M60","M62","M70","M74","M77","M78A","M78B","M78C,""M78D","M78E","H2","H3","H4","H5","H6","H7","H8","H9","H9A","H0","H10","H33","H52","H53","H54","H55","H56","H60","H61","H62","H63","H64","H65","H66","HIDXA","H11","H22","H31","H31B","H32Y","H34","H37A","H37B","H37C","H37D","H37DA","H37E","H37AA","H37AB","H37I","H37J","H37L","H37M","H37N","H37O","H37X","H37Y","H42","H43","HWIDX","HW1","HW2","HW3","HW4","HW5","HW6","HW7","HW8","HW9","HW10","HW11","HW12","HW13","HW70","HW71","HW72","HW73","IDXML","ML0","SWFEDUC1","SWFREGUR","S207BB","S301A","S708A","S709A","S908A","S908B","S924A","S932F","S1107A","S1107E","S1107","S1107G","S1107J","S1110A","SV005","IDX94","S409A","S413AA","S413AB","S413AC"]

from sklearn.experimental import enable_iterative_imputer
from sklearn.impute import IterativeImputer

imp = IterativeImputer(missing_values=np.nan, sample_posterior=False, 
                                 max_iter=10, tol=0.001, 
                                 n_nearest_features=4, initial_strategy='median')

x = list(dfx.columns)

imp.fit(dfx) 
imputed_data = pd.DataFrame(data=imp.transform(dfx), 
                             columns=[x],
                             dtype='int')

imputed_data.head()

imputed_data.shape

imputed_data.isnull().sum()

imputed_data.to_csv("infants_updated_2.csv")

"""### Finally I have successfully cleaned my data using scikit learn imputations.Now I will apply PCA to reduce the dimensionality of the data"""

from sklearn.preprocessing import StandardScaler
sc_X = StandardScaler()
dfx =sc_X.fit_transform(dfx)

dfx.shape

"""**Applying Principal Component Analysis ***"""

from sklearn.decomposition import PCA
pca = PCA(n_components = 2)
pca.fit(dfx)

# transform data onto the first two principal components
X_pca = pca.transform(dfx)
print("Original shape:{}".format(dfx.shape))
print("Original shape:{}".format(X_pca.shape))

principalDf = pd.DataFrame(data = X_pca, columns = ['principal component 1', 'principal component 2'])

principalDf.head()

PCA_fr = pd.concat([principalDf,dfy[["target"]]],axis =1)

PCA_fr.head()

#plotting the components
fig = plt.figure(figsize = (8,8))
plt.xlabel('Principal Component - 1',fontsize=20)
plt.ylabel('Principal Component - 2',fontsize=20)
plt.title("Principal Component Analysis of infant Dataset",fontsize=20)
targets = [1, 0]
colors = ['r', 'g']
for target, color in zip(targets,colors):
  indicesToKeep = PCA_fr["target"] == target
  plt.scatter(PCA_fr.loc[indicesToKeep, 'principal component 1'] 
              ,PCA_fr.loc[indicesToKeep, 'principal component 2']
              , c = color
              , s = 50)
plt.legend(targets,prop={'size': 15})

var = pca.explained_variance_ratio_

print("Explained Variation per principal component: {}".format(var))

"""We can see that first principal component only holds 5% of information while second principal component only holds 3% of information.Thus 93% of information was lost while reducing the dimensionality from 489 to 2.We need to plot Scree plot to."""

pca = PCA(n_components=489)
pca.fit(dfx)

#Explained variance
var = pca.explained_variance_ratio_
var1=np.cumsum(np.round(var, decimals=4)*100)
print(var1)

plt.ylabel("Eigen Value =Propotion of Varaince Explained")
plt.xlabel("Number of Principal Component")
plt.title("Scree plot")
plt.plot(var,"bo")

plt.ylabel("CumulativePropotion of Varaince Explained")
plt.xlabel("Number of Principal Components")
plt.title("Cumulative Scree plot")
plt.plot(var1,)

#choosing k = 321 as 95% of variance is retained at that value AND applying to whole data.
pca = PCA(n_components=321)
#pca.fit(dfx)
X1=pca.fit_transform(dfx)
var = pca.explained_variance_ratio_
var1=np.cumsum(np.round(var, decimals=4)*100)
print(var1)

X1

#converting it to data frame
dfx = pd.DataFrame(data = X1)
print("The shape of data frame after applying PCA is : {}".format(dfx.shape))

#rounding data frame to 2 dp
dfx = dfx.round(2)

dfx.head()

#saving the updated frame
from google.colab import files
dfx.to_csv('infants_updated_4.csv')
files.download('infants_updated_4.csv')

"""**Applying Logistic Regression on data reduced by  PCA**"""

# fix random seed for reproducibility
seed = 7
np.random.seed(seed)

dfx =pd.read_csv("infants_updated_4.csv")
dfx = dfx.drop(["Unnamed: 0"],axis =1)
dfy =pd.read_csv("target.csv")
dfy = dfy.drop(["Unnamed: 0"],axis =1)

from sklearn.model_selection import train_test_split
X_train, X_test, Y_train, Y_test = train_test_split(dfx,dfy, test_size=0.2)

from sklearn.preprocessing import StandardScaler
sc_X = StandardScaler()
X_train = sc_X.fit_transform(X_train)
X_test = sc_X.transform(X_test)

X_train = pd.DataFrame(X_train)
X_test = pd.DataFrame(X_test)

from sklearn.linear_model import LogisticRegression
logisticRegr = LogisticRegression(solver = 'lbfgs')
logisticRegr.fit(X_train, Y_train)

from sklearn.metrics import accuracy_score
y_hat = logisticRegr.predict(X_test)
accuracy = accuracy_score(Y_test,y_hat)
print("Logistic Regression Accuracy on test data is : {}".format(accuracy))

from sklearn.metrics import classification_report
print("The scores by logistic regression model are \n",classification_report(Y_test, y_hat,
                            target_names=["not survived", "survived"]))

from sklearn.metrics import roc_auc_score
from sklearn.metrics import confusion_matrix
auc = roc_auc_score(Y_test, y_hat)
print('ROC AUC: %f' % auc)
# confusion matrix
matrix = confusion_matrix(Y_test, y_hat)
print(matrix)

from sklearn.metrics import plot_roc_curve
roc_curve = plot_roc_curve(logisticRegr, X_test, Y_test)

from sklearn.metrics import roc_curve
fpr_lr, tpr_lr, thresholds_lr = roc_curve(Y_test, y_hat)
from sklearn.metrics import auc
auc_lr = auc(fpr_lr, tpr_lr)

"""## Building a Deep Learning Model
*I have allowed all the parameters for model to learn instead of reduced data as it gave a better accuracy in deep learning.*
"""

# fix random seed for reproducibility
seed = 7
np.random.seed(seed)

dfx =pd.read_csv("infants_updated_2.csv")
dfx = dfx.drop(["Unnamed: 0"],axis =1)
dfy =pd.read_csv("target.csv")
dfy = dfy.drop(["Unnamed: 0"],axis =1)

dfx.head()

dfy.head()

from sklearn.model_selection import train_test_split
X_train, X_test, Y_train, Y_test = train_test_split(dfx,dfy, test_size=0.2)

from sklearn.preprocessing import StandardScaler
sc_X = StandardScaler()
X_train = sc_X.fit_transform(X_train)
X_test = sc_X.transform(X_test)

X_train = pd.DataFrame(X_train)
X_test = pd.DataFrame(X_test)

X_train.shape

X_train.head()

#validating my approach
x_val = X_train[:600]
partial_x_train = X_train[600:]
y_val = Y_train[:600]
partial_y_train = Y_train[600:]

#defining my model
model = models.Sequential()
model.add(layers.Dense(256, activation='relu', input_shape=(X_train.shape[1],)))
model.add(layers.Dense(256, activation='relu'))
model.add(layers.Dense(1, activation='sigmoid'))
#compiling my model
model.compile(optimizer='rmsprop',
loss='binary_crossentropy',
metrics=['acc'])
#training my model
history = model.fit(partial_x_train,partial_y_train,epochs=20,batch_size=100,
validation_data=(x_val, y_val))

history_dict = history.history
history_dict.keys()

#Plotting the training and validation loss
import matplotlib.pyplot as plt
plt.plot(history.history['loss'] ,'bo')
plt.plot(history.history['val_loss'],'b',)
plt.title('Training and validation loss')
plt.ylabel('Loss')
plt.xlabel('Epochs')
plt.legend(['training loss', 'validation loss'], loc='upper right')
plt.show()

plt.plot(history.history['acc'],"bo")
plt.plot(history.history['val_acc'],"b")
plt.title('Training and validation accuracy')
plt.ylabel('Loss')
plt.xlabel('Epochs')
plt.legend(['Training_acc', 'Validation_acc'], loc='lower right')
plt.show()

"""**Predicting on Test Set**"""

predictions = model.evaluate(x=X_test,y=Y_test,verbose =1)
print(predictions)

"""**Since the model shows a 99% accuracy it is a generalized model.Let's visualize the architecture.**"""

from keras.utils.vis_utils import plot_model
plot_model(model, to_file='model_plot.png', show_shapes=True, show_layer_names=True)

#evaluation metrics
# predict probabilities for test set
yhat_probs = model.predict(X_test, verbose=0)
# predict crisp classes for test set
yhat_classes = model.predict_classes(X_test, verbose=0)

from sklearn.metrics import classification_report
print("The scores by deep learning model are \n",classification_report(Y_test, yhat_classes,
                            target_names=["not survived", "survived"]))

from sklearn.metrics import roc_auc_score
from sklearn.metrics import confusion_matrix
auc = roc_auc_score(Y_test, yhat_probs)
print('ROC AUC: %f' % auc)
# confusion matrix
matrix = confusion_matrix(Y_test, yhat_classes)
print(matrix)

from sklearn.metrics import roc_curve
y_pred_keras = model.predict(X_test).ravel()
fpr_keras, tpr_keras, thresholds_keras = roc_curve(Y_test, yhat_probs)

from sklearn.metrics import auc
auc_keras = auc(fpr_keras, tpr_keras)

plt.figure(1)
plt.plot([0, 1], [0, 1], 'k--')
plt.plot(fpr_keras, tpr_keras, label='Keras (area = {:.3f})'.format(auc_keras))
plt.xlabel('False positive rate')
plt.ylabel('True positive rate')
plt.title('ROC curve')
plt.legend(loc='best')
plt.show()

"""**Building a Bayesian Model with Naive Bayes Classifier**"""

# fix random seed for reproducibility
seed = 7
np.random.seed(seed)

dfx =pd.read_csv("infants_updated_2.csv")
dfy =pd.read_csv("target.csv")
dfy = dfy.drop(["Unnamed: 0"],axis =1)

from sklearn.model_selection import train_test_split
X_train, X_test, Y_train, Y_test = train_test_split(dfx,dfy, test_size=0.2)

from sklearn.preprocessing import StandardScaler
sc_X = StandardScaler()
X_train = sc_X.fit_transform(X_train)
X_test = sc_X.transform(X_test)

X_train = pd.DataFrame(X_train)
X_test = pd.DataFrame(X_test)

X_train.head()

#importing the naive Bayes
from sklearn.naive_bayes import GaussianNB
#intiating the model
model = GaussianNB()
# Train the model using the training sets
model.fit(X_train,Y_train)

y_hat = model.predict(X_test)

from sklearn.metrics import accuracy_score
accuracy = accuracy_score(Y_test,y_hat)
print("Naive Bayes Accuracy on test data is : {}".format(accuracy))

#evaluating the model
from sklearn.metrics import confusion_matrix
print(confusion_matrix(Y_test, y_hat))

from sklearn.metrics import classification_report
print("The scores by Naive Bayes are \n",classification_report(Y_test, y_hat,
                            target_names=["not survived", "survived"]))

from sklearn.metrics import roc_auc_score
auc = roc_auc_score(Y_test, y_hat)
print('ROC AUC: %f' % auc)

from sklearn.metrics import plot_roc_curve
roc_curve = plot_roc_curve(model, X_test, Y_test)

from sklearn.metrics import roc_curve
fpr_nv, tpr_nv, thresholds_nv = roc_curve(Y_test, y_hat)
from sklearn.metrics import auc
auc_nv = auc(fpr_nv, tpr_nv)

"""**Applying Random Forest**

*I will be providing unscaled data as Decision Tree algorithms gave good accuracy without scaling*
"""

# fix random seed for reproducibility
seed = 7
np.random.seed(seed)

dfx =pd.read_csv("infants_updated_2.csv")
dfx = dfx.drop(["Unnamed: 0"],axis =1)
dfy =pd.read_csv("target.csv")
dfy = dfy.drop(["Unnamed: 0"],axis =1)

from sklearn.model_selection import train_test_split
X_train, X_test, Y_train, Y_test = train_test_split(dfx,dfy, test_size=0.2,random_state = 0)

X_train.head()

Y_train.head()

#training with n = 100
from sklearn.ensemble import RandomForestClassifier
forest = RandomForestClassifier(n_estimators=100, random_state=0,oob_score = True,bootstrap = True)
forest.fit(X_train, Y_train)

print("Accuracy on training set: {:.3f}".format(forest.score(X_train, Y_train)))
print("Accuracy on test set: {:.3f}".format(forest.score(X_test, Y_test)))

y_hat = forest.predict(X_test)

#evaluating the model
from sklearn.metrics import confusion_matrix
print(confusion_matrix(Y_test, y_hat))

from sklearn.metrics import classification_report
print("The scores by Random Forest are \n",classification_report(Y_test, y_hat,
                            target_names=["not survived", "survived"]))

from sklearn.metrics import roc_auc_score
auc = roc_auc_score(Y_test, y_hat)
print('ROC AUC: %f' % auc)

from sklearn.metrics import plot_roc_curve
roc_curve = plot_roc_curve(forest, X_test, Y_test)

from sklearn.metrics import roc_curve
fpr_rf, tpr_rf, thresholds_rf = roc_curve(Y_test, y_hat)
from sklearn.metrics import auc
auc_rf = auc(fpr_rf, tpr_rf)

#Seeing the Feature Importance by Mean Decrease in Impurity (MDI)
features = X_train.columns
importances = forest.feature_importances_
indices = np.argsort(importances)
plt.figure(figsize=(20,100))
plt.title('Feature Importances')
plt.barh(range(len(indices)), importances[indices], color='b', align='center')
plt.yticks(range(len(indices)), [features[i] for i in indices])
plt.xlabel('Relative Importance')
plt.show()

#making a data frame for 20 most important features
most_imp = pd.DataFrame({"Coded Feature names" : ["V404","HW2","M5","V219","V206","M19","M4","H0","HW1","V207","H4","HW70","H10","H2","HW10","HW8","HW73","V137","HW9","H61"],"Actual Feature Names" : ["Currently breastfeeding","Child's weight in kilograms (1 decimal)","Months of breastfeeding","Living children + current pregnancy","Sons who have died","Birth weight in kilograms (3 decimals) of Child","Duration of breastfeeding","Received POLIO 0","Child's age in months","Daughters who have died","Received POLIO 1","Height/Age standard deviation (new WHO) of Child","Ever had vaccination","Received BCG","Weight/Height percentile of Child","Weight/Age standard deviation of Child","BMI standard deviation (new WHO)","Number of children 5 and under in household ","Weight/Age percent of ref. median","Received Hepatitis B1"] })

print("Top 20 features extracted by Random Forest are \n: {}".format(most_imp))

#training a model with top 20 features
df =  dfx.loc[:,["V404","HW2","M5","V219","V206","M19","M4","H0","HW1","V207","H4","HW70","H10","H2","HW10","HW8","HW73","V137","HW9","H61"]]
print(df.head())
print(df.shape)

from sklearn.model_selection import train_test_split
X_train, X_test, Y_train, Y_test = train_test_split(df,dfy, test_size=0.2)

#training with n = 100
from sklearn.ensemble import RandomForestClassifier
forest = RandomForestClassifier(n_estimators=100, random_state=0,oob_score = True,bootstrap = True)
forest.fit(X_train, Y_train)

print("Accuracy on training set: {:.3f}".format(forest.score(X_train, Y_train)))
print("Accuracy on test set: {:.3f}".format(forest.score(X_test, Y_test)))

#training a model with top 10 features
df =  dfx.loc[:,["V404","HW2","M5","V219","V206","M19","M4","H0","HW1","V207"]]
print(df.head())
print(df.shape)

from sklearn.model_selection import train_test_split
X_train, X_test, Y_train, Y_test = train_test_split(df,dfy, test_size=0.2)

#training with n = 100
from sklearn.ensemble import RandomForestClassifier
forest = RandomForestClassifier(n_estimators=100, random_state=0,oob_score = True,bootstrap = True)
forest.fit(X_train, Y_train)

print("Accuracy on training set: {:.3f}".format(forest.score(X_train, Y_train)))
print("Accuracy on test set: {:.3f}".format(forest.score(X_test, Y_test)))

#training a model with top 5 features
dfX =  dfx.loc[:,["V404","HW2","M5","V219","V206"]] 
print(dfX.head())
print(dfX.shape)

dfy.shape

from sklearn.model_selection import train_test_split
X_train, X_test, Y_train, Y_test = train_test_split(dfX,dfy, test_size=0.2,random_state =0)

X_train.shape

#training with n = 100
from sklearn.ensemble import RandomForestClassifier
forest = RandomForestClassifier(n_estimators=100, random_state=0,oob_score = True,bootstrap = True)
forest.fit(X_train, Y_train)

print("Accuracy on training set: {:.3f}".format(forest.score(X_train, Y_train)))
print("Accuracy on test set: {:.3f}".format(forest.score(X_test, Y_test)))

"""**With only 5 features,random forest gives an accuracy of 99%**

**Building Support Vector Machine Classifier**
"""

# fix random seed for reproducibility
seed = 12
np.random.seed(seed)

dfx =pd.read_csv("infants_updated_4.csv")
dfx = dfx.drop(["Unnamed: 0"],axis =1)
dfy =pd.read_csv("target.csv")
dfy = dfy.drop(["Unnamed: 0"],axis =1)

dfx.head()

from sklearn.model_selection import train_test_split
X_train, X_test, Y_train, Y_test = train_test_split(dfx,dfy, test_size=0.2,random_state =0)

X_train.head()

"""*I will be chaining the processing steps by Pipeline class in Sci-kit learn*"""

from sklearn.pipeline import Pipeline
from sklearn.preprocessing import StandardScaler
from sklearn.svm import SVC
pipe = pipe = Pipeline([("scaler",StandardScaler()), ("svm", SVC())])

pipe.fit(X_train,Y_train)

print("Test Score: {:.2f}.".format(pipe.score(X_test,Y_test)))

"""*Fine tuning the parameter using Grid Search*"""

parameter_grid = {"svm__C":[0.001, 0.01, 0.1, 1, 10, 100],
                  "svm__gamma":[0.001, 0.01, 0.1, 1, 10, 100]}

"""*Grid Search with Accuracy*"""

from sklearn.model_selection import GridSearchCV
Grid = GridSearchCV(pipe,param_grid =parameter_grid,cv=5)
Grid.fit(X_train, np.ravel(Y_train,order='C'))
print("Best Cross-Validation Accuracy: {:.2f}".format(Grid.best_score_))
print("Test set score: {:.2f}".format(Grid.score(X_test, Y_test)))
print("Best parameters: {}".format(Grid.best_params_))

"""Grid Search with AUC"""

from sklearn.model_selection import GridSearchCV
from sklearn.metrics import roc_auc_score
Grid = GridSearchCV(pipe,param_grid =parameter_grid,cv=5,scoring="roc_auc")
Grid.fit(X_train, np.ravel(Y_train,order='C'))
print("Best Cross-Validation Accuracy: {:.2f}".format(Grid.best_score_))
print("Test set AUC: {:.3f}".format(
    roc_auc_score(Y_test, Grid.decision_function(X_test))))
print("Test set accuracy: {:.2f}".format(Grid.score(X_test, Y_test)))
print("Best parameters: {}".format(Grid.best_params_))

classification_report(Grid.best_estimator_.predict(X_test), Y_test)

"""Building SVM with tuned Parameters"""

# fix random seed for reproducibility
seed = 88
np.random.seed(seed)
dfx =pd.read_csv("infants_updated_4.csv")
dfx = dfx.drop(["Unnamed: 0"],axis =1)
dfy =pd.read_csv("target.csv")
dfy = dfy.drop(["Unnamed: 0"],axis =1)
from sklearn.model_selection import train_test_split
X_train, X_test, Y_train, Y_test = train_test_split(dfx,dfy, test_size=0.2,random_state =108)

from sklearn.preprocessing import StandardScaler
sc_X = StandardScaler()
X_train = sc_X.fit_transform(X_train)
X_test = sc_X.transform(X_test)

from sklearn.svm import SVC
support_vector = SVC(kernel='rbf',C = 10,gamma = 0.001)
support_vector.fit(X_train,Y_train)
y_hat=support_vector.predict(X_test)

print("Accuracy on training set: {:.3f}".format(support_vector.score(X_train, Y_train)))
print("Accuracy on test set: {:.3f}".format(support_vector.score(X_test, Y_test)))

#evaluating the model
from sklearn.metrics import confusion_matrix
print(confusion_matrix(Y_test, y_hat))
from sklearn.metrics import classification_report
print("The scores by Support Vector Machine are \n",classification_report(Y_test, y_hat,
                            target_names=["not survived", "survived"]))

from sklearn.metrics import roc_auc_score
auc = roc_auc_score(Y_test, y_hat)
print('ROC AUC: %f' % auc)
from sklearn.metrics import plot_roc_curve
roc_curve = plot_roc_curve(support_vector, X_test, Y_test)
from sklearn.metrics import roc_curve
fpr_svc, tpr_svc, thresholds_svc = roc_curve(Y_test, y_hat)
from sklearn.metrics import auc
auc_svc = auc(fpr_svc, tpr_svc)

"""**Training a Light GBM Classifier**"""

# fix random seed for reproducibility
seed = 7
np.random.seed(seed)

dfx =pd.read_csv("infants_updated_2.csv")
dfx = dfx.drop(["Unnamed: 0"],axis =1)
dfy =pd.read_csv("target.csv")
dfy = dfy.drop(["Unnamed: 0"],axis =1)

from sklearn.model_selection import train_test_split
X_train, X_test, Y_train, Y_test = train_test_split(dfx,dfy, test_size=0.2,random_state =0)

X_train.head()

from lightgbm import LGBMClassifier
#defining model
model = LGBMClassifier()
trained = model.fit(X_train,Y_train)
y_hat=model.predict(X_test)
print("Accuracy on training set: {:.3f}".format(model.score(X_train, Y_train)))
print("Accuracy on test set: {:.3f}".format(model.score(X_test, Y_test)))

#evaluating the model
from sklearn.metrics import confusion_matrix
print(confusion_matrix(Y_test, y_hat))
from sklearn.metrics import classification_report
print("The scores by LGBM  are \n",classification_report(Y_test, y_hat,
                            target_names=["not survived", "survived"]))
from sklearn.metrics import roc_auc_score
auc = roc_auc_score(Y_test, y_hat)
print('ROC AUC: %f' % auc)
from sklearn.metrics import plot_roc_curve
roc_curve = plot_roc_curve(model, X_test, Y_test)
from sklearn.metrics import roc_curve
fpr_lgbmfull, tpr_lgbmfull, thresholds_lgbmfull = roc_curve(Y_test, y_hat)
from sklearn.metrics import auc
auc_lgbmfull = auc(fpr_lgbmfull, tpr_lgbmfull)



features = X_train.columns
importances = model.feature_importances_
indices = np.argsort(importances)
plt.figure(figsize=(20,100))
plt.title('Feature Importances')
plt.barh(range(len(indices)), importances[indices], color='b', align='center')
plt.yticks(range(len(indices)), [features[i] for i in indices])
plt.xlabel('Relative Importance')
plt.show()

most_imp_LGBM = pd.DataFrame({"Coded Feature names" : ["V005","V404","HW2","V191","V191A","H4","V221","AWFACTW","B11","HW3"],
                         "Description": ["Women's individual sample weight (6 decimals)","Currently breastfeeding","Child's weight in kilograms (1 decimal)","Wealth index factor score combined (5 decimals)","Wealth index factor score for urban/rural (5 decimals)","Received POLIO 1","Marriage to first birth interval (months)","All woman factor - wealth index","Preceding birth interval (months)","Child's height in centimeters (1 decimal)"]})

print("The most important features found by Light GBM Gradient Boosting are \n: {}".format(most_imp_LGBM))

"""*Comparing feature extraction of Random Forest and LGBM*"""

most_imp.head(10)

most_imp_LGBM

"""**Insights using Both Classifiers:**
1.   V404 that is whether a child is currently breastfeeding is important as appears in both feature_imp

2.   HW2 that is Child's weight in kg is important determining survival of infant

**After training some models now I will be selecting some features and will try to make my models more simpler**
"""

from sklearn.model_selection import train_test_split
X_train, X_test, Y_train, Y_test = train_test_split(dfx,dfy, test_size=0.2,random_state =0)

X_train.shape

"""*Implementing simple Sequential Forward Selection*"""

from sklearn.ensemble import RandomForestClassifier

from mlxtend.feature_selection import SequentialFeatureSelector as SFS

sfs1 = SFS(RandomForestClassifier(n_estimators=100,random_state=0,n_jobs=-1),
           k_features=6, 
           forward=True, 
           floating=False, 
           verbose=2,
           scoring='accuracy',
           n_jobs = -1,
           cv =5)

sfs1 = sfs1.fit(X_train, Y_train)

#indices for five best features
print("Best 5 feature indexes using Random Forest and Forward Selection are: {}".format(sfs1.k_feature_idx_))
print("Best 5 feature using Random Forest and Forward Selection are: {}".format(sfs1.k_feature_names_))

pd.DataFrame.from_dict(sfs1.get_metric_dict()).T

sfs1.k_score_

from mlxtend.plotting import plot_sequential_feature_selection as plot_sfs
plot_sfs(sfs1.get_metric_dict(),kind = "std_dev")

dfx =pd.read_csv("infants_updated_2.csv")
dfx = dfx.drop(["Unnamed: 0"],axis =1)
dfy =pd.read_csv("target.csv")
dfy = dfy.drop(["Unnamed: 0"],axis =1)

"""*Training on LGBM with top 10 features*"""

dfX =  dfx.loc[:,["V005","V404","HW2","V191","V191A","H4","V221","AWFACTW","B11","HW3"]] 
print(dfX.head())
print(dfX.shape)

from sklearn.model_selection import train_test_split
X_train, X_test, Y_train, Y_test = train_test_split(dfX,dfy, test_size=0.2,random_state =0)

X_train.head()

from lightgbm import LGBMClassifier
#defining model
model = LGBMClassifier(learning_rate= 0.01, max_depth = 5, n_estimators= 1000, num_leaves= 10)
trained = model.fit(X_train,Y_train)
y_hat=model.predict(X_test)
print("Accuracy on training set: {:.3f}".format(model.score(X_train, Y_train)))
print("Accuracy on test set: {:.3f}".format(model.score(X_test, Y_test)))

#evaluating the model
from sklearn.metrics import confusion_matrix
print(confusion_matrix(Y_test, y_hat))
from sklearn.metrics import classification_report
print("The scores by LGBM with 10 features  are \n",classification_report(Y_test, y_hat,
                            target_names=["not survived", "survived"]))
from sklearn.metrics import roc_auc_score
auc = roc_auc_score(Y_test, y_hat)
print('ROC AUC: %f' % auc)
from sklearn.metrics import plot_roc_curve
roc_curve = plot_roc_curve(model, X_test, Y_test)
from sklearn.metrics import roc_curve
fpr_lgbm10, tpr_lgbm10, thresholds_lgbm10 = roc_curve(Y_test, y_hat)
from sklearn.metrics import auc
auc_lgbm10 = auc(fpr_lgbm10, tpr_lgbm10)



"""*Training on Random Forest with top 10 features*"""

dfX =  dfx.loc[:,["V404","HW2","M5","V219","V206","M19","M4","H0","HW1","V207"]]
print(dfX.head())
print(dfX.shape)

from sklearn.model_selection import train_test_split
X_train, X_test, Y_train, Y_test = train_test_split(dfX,dfy, test_size=0.2,random_state =0)

#training with n = 100
from sklearn.ensemble import RandomForestClassifier
forest = RandomForestClassifier(n_estimators=100, random_state=0,oob_score = True,bootstrap = True)
forest.fit(X_train, Y_train)
y_hat=model.predict(X_test)

print("Accuracy on training set: {:.3f}".format(forest.score(X_train, Y_train)))
print("Accuracy on test set: {:.3f}".format(forest.score(X_test, Y_test)))

#evaluating the model
from sklearn.metrics import confusion_matrix
print(confusion_matrix(Y_test, y_hat))
from sklearn.metrics import classification_report
print("The scores by RF with 10 features  are \n",classification_report(Y_test, y_hat,
                            target_names=["not survived", "survived"]))
from sklearn.metrics import roc_auc_score
auc = roc_auc_score(Y_test, y_hat)
print('ROC AUC: %f' % auc)
from sklearn.metrics import plot_roc_curve
roc_curve = plot_roc_curve(forest, X_test, Y_test)
from sklearn.metrics import roc_curve
fpr_rf10, tpr_rf10, thresholds_rf10 = roc_curve(Y_test, y_hat)
from sklearn.metrics import auc
auc_rf10 = auc(fpr_rf10, tpr_rf10)

"""**Getting a test set accuracy of 99 % in both Classifiers but let's see if we can increase by fine tuning the parameters of LGBM**"""

param_grid = {'n_estimators': [100, 500, 1000,5000],
              'max_depth': [-2,-1,5,7],
              'num_leaves' : [10,100,500,],
              'learning_rate' : [0.001,0.01,0.1,0.5],
}

from sklearn.model_selection import GridSearchCV
from sklearn.svm import SVC
from lightgbm import LGBMClassifier
grid_search = GridSearchCV(LGBMClassifier(), param_grid, cv=5)

from sklearn.model_selection import train_test_split
X_train, X_test, Y_train, Y_test = train_test_split(
    dfX, dfy, random_state=0)

X_train.head()

grid_search.fit(X_train, Y_train)

print("Test set score: {:.2f}".format(grid_search.score(X_test, Y_test)))

print("Best parameters: {}".format(grid_search.best_params_))
print("Best cross-validation score: {:.2f}".format(grid_search.best_score_))

"""Finally Building The LGBM Classifier with tuned in parameters"""

dfx =pd.read_csv("infants_updated_2.csv")
dfx = dfx.drop(["Unnamed: 0"],axis =1)
dfy =pd.read_csv("target.csv")
dfy = dfy.drop(["Unnamed: 0"],axis =1)

dfX =  dfx.loc[:,["V005","V404","HW2","V191","V191A"]] 
print(dfX.head())
print(dfX.shape)

from sklearn.model_selection import train_test_split
X_train, X_test, Y_train, Y_test = train_test_split(
    dfX, dfy, random_state=0)

#defining model
model = LGBMClassifier(learning_rate= 0.01, max_depth = 5, n_estimators= 1000, num_leaves= 10)
trained = model.fit(X_train,Y_train)
pred_LGBM = model.predict(X_test)
print("Accuracy on training set: {:.3f}".format(model.score(X_train, Y_train)))
print("Accuracy on test set: {:.3f}".format(model.score(X_test, Y_test)))

"""*Evaluation*"""

from sklearn.metrics import confusion_matrix
confusion = confusion_matrix(Y_test, pred_LGBM)
print("Confusion matrix:\n{}".format(confusion))

from sklearn.metrics import classification_report
print(classification_report(Y_test, pred_LGBM,
                            target_names=["not survived", "survived"]))

from sklearn.metrics import plot_roc_curve
roc_curve = plot_roc_curve(model, X_test, Y_test)

from sklearn.metrics import roc_auc_score
auc = roc_auc_score(Y_test, pred_LGBM)
print('ROC AUC: %f' % auc)
from sklearn.metrics import plot_roc_curve
roc_curve = plot_roc_curve(model, X_test, Y_test)
from sklearn.metrics import roc_curve
fpr_lgbm, tpr_lgbm, thresholds_lgbm = roc_curve(Y_test, pred_LGBM)
from sklearn.metrics import auc
auc_lgbm = auc(fpr_lgbm, tpr_lgbm)

"""*Now let's fine tune Random Forest Parameters and check whether we can increase its accuracy.But before that I want to try to engineer a feature which will take sum of Daughters and Sons death because they were considered important by Random forest and will drop seperate ones*"""

dfx =pd.read_csv("infants_updated_2.csv")
dfx = dfx.drop(["Unnamed: 0"],axis =1)
dfy =pd.read_csv("target.csv")
dfy = dfy.drop(["Unnamed: 0"],axis =1)

dfx.shape

dfx['Tchild_death'] = dfx.loc[:,['V206','V207']].sum(axis=1)

dfx.shape

dfx.loc[:,["V206","V207","Tchild_death"]]

dfx = dfx.drop(['V206','V207'],axis =1)

dfx.shape

from sklearn.model_selection import train_test_split
X_train, X_test, Y_train, Y_test = train_test_split(dfx,dfy, test_size=0.2)

#training with n = 100
from sklearn.ensemble import RandomForestClassifier
forest = RandomForestClassifier(n_estimators=100, random_state=0,oob_score = True,bootstrap = True)
forest.fit(X_train, Y_train)

print("Accuracy on training set: {:.3f}".format(forest.score(X_train, Y_train)))
print("Accuracy on test set: {:.3f}".format(forest.score(X_test, Y_test)))

#Seeing the Feature Importance by Mean Decrease in Impurity (MDI)
features = X_train.columns
importances = forest.feature_importances_
indices = np.argsort(importances)
plt.figure(figsize=(20,100))
plt.title('Feature Importances')
plt.barh(range(len(indices)), importances[indices], color='b', align='center')
plt.yticks(range(len(indices)), [features[i] for i in indices])
plt.xlabel('Relative Importance')
plt.show()

"""*Thats's great seeing the new feature sitting on top.Now let's build classifier with top 10 features *"""

dfX = dfx.loc[:,["V404",'Tchild_death',"HW2","M5","V219","H0","HW70","H4","M19","M4"]]

dfX.head()

dfX.columns= ["Currently Breastfeeding","Number of children died before","Child's weight in kg","Months of breastfeeding","Living Children + current pregnancy","Recieved Polio","Height/Age standard deviation","Recieved Polio 1","Birth weight in kg of Child","Ever Breastfed"]

dfX.head()

from sklearn.model_selection import train_test_split
X_train, X_test, Y_train, Y_test = train_test_split(dfX,dfy, test_size=0.2)

#training with n = 100
from sklearn.ensemble import RandomForestClassifier
forest = RandomForestClassifier(n_estimators=100, random_state=0,oob_score = True,bootstrap = True)
forest.fit(X_train, Y_train)

print("Accuracy on training set: {:.3f}".format(forest.score(X_train, Y_train)))
print("Accuracy on test set: {:.3f}".format(forest.score(X_test, Y_test)))

#saving the updated frame
from google.colab import files
dfX.to_csv('infants_final.csv')
files.download('infants_final.csv')

"""**THAT'S IT,with only 10 features our model gives 100 % accuracy on test data.Now it's time to test our model on completely New Data Set.This is a survey result of Nepal which is uploaded below**

**Uploading new data set**

*The data uploaded below is reduced to test only on the features which model is built and hence complete data is not uploaded and is uploaded of infants only*
"""

#Importing the data 
from google.colab import files
files.upload()

df =pd.read_csv("infants_updated_nepal_1.csv")
df = df.drop(["Unnamed: 0"],axis =1)

df.head()

df.shape

df.replace( {'V404': {"Yes": 1}},inplace = True)
df.replace( {'V404': {"No": 0}},inplace = True)

df.V404.value_counts()

#summing up the death of Son and daughters
df['Tchild_death'] = df.loc[:,['V206','V207']].sum(axis=1)
df = df.drop(['V206','V207'],axis =1)

df = df.drop(["B2","B1","V007","V006"],axis =1)

df.head()

#seperating Target variable
df.replace( {'B5': {"Yes": 1}},inplace = True)

df.replace( {'H4': {"Vaccination date on card": 1}},inplace = True)
df.replace( {'H4': {"No": 0}},inplace = True)
df.replace( {'H4': {"Reported by mother": 1}},inplace = True)
df.replace( {'H4': {"Vaccination marked on card": 1}},inplace = True)
df.replace( {'H4': {"Don't know": np.nan}},inplace = True)

df.M4.value_counts()

df.replace( {'M4': {"Still breastfeeding": 1}},inplace = True)
df.replace( {'M4': {"Ever breastfed, not currently breastfeeding": 0}},inplace = True)
df.replace( {'M4': {"Never breastfed": 0}},inplace = True)

df.M4.value_counts()

df.head()

df= df.dropna(how ="any",axis =0)

df.replace( {'M5': {"Ever breastfed, not currently breastfeeding": 0}},inplace = True)
df.replace( {'M5': {"Never breastfed": 0}},inplace = True)

df = df[df.HW70 != "Flagged cases"]

df.replace( {'M19': {"Don't know": np.nan}},inplace = True)
df.replace( {'M19': {"Not weighed at birth": np.nan}},inplace = True)

df = df.astype("float64")

df= df.dropna(how ="any",axis =0)

x= list(df.columns)

df[x] = df[x].applymap(np.int64)

#adding a column of received polio as 0 as no sample point had recieved polio
df["H0"] = 0

df.head(5)

#arranging in order 
cols = ["V404",'Tchild_death',"HW2","M5","V219","H0","HW70","H4","M19","M4"]
df = df[cols]

df.head()

#seperating target var
df_target_Nep = pd.DataFrame(data =df.B5)

df_target_Nep

print(df.shape)
print(df_target_Nep.shape)

"""*Finally after Preprocess data is ready to test for the model*"""

print("Accuracy on test set: {:.3f}".format(forest.score(df, df_target_Nep)))

"""**As it can be seen system is predicting accuratly on Nepals infants data,now the model is ready for deployment**

**Comparing different algrotithms (Visualization)**
"""

import pandas
import matplotlib.pyplot as plt
from sklearn import model_selection
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from lightgbm import LGBMClassifier
from sklearn.svm import SVC
from sklearn.neural_network import MLPClassifier
from sklearn.naive_bayes import GaussianNB

# prepare configuration for cross validation test harness
seed = 7
# prepare models
models = []
models.append(('LR', LogisticRegression(random_state=7)))
models.append(('RF', RandomForestClassifier(random_state=7)))
models.append(('NVB', GaussianNB()))
models.append(('LGBM', LGBMClassifier(random_state=7)))
models.append(('SVM', SVC(random_state=7)))
models.append(("NN",MLPClassifier(random_state=7)))
# evaluate each model in turn
results = []
names = []
scoring = 'accuracy'
for name, model in models:
	kfold = model_selection.KFold(n_splits=5, random_state=seed)
	cv_results = model_selection.cross_val_score(model, dfx, dfy, cv=kfold, scoring=scoring)
	results.append(cv_results)
	names.append(name)
	msg = "%s: %f (%f)" % (name, cv_results.mean(), cv_results.std())
	print(msg)

# boxplot algorithm comparison
fig = plt.figure()
fig.suptitle('Algorithm Comparison')
ax = fig.add_subplot(111)
plt.boxplot(results)
ax.set_xticklabels(names)
plt.show()

plt.figure(1)
plt.plot([0, 1], [0, 1], 'k--')
plt.plot(fpr_keras, tpr_keras, label='Keras (area = {:.3f})'.format(auc_keras))
plt.plot(fpr_lgbmfull, tpr_lgbmfull, label='LGBM (area = {:.3f})'.format(auc_lgbmfull))
plt.plot(fpr_lr, tpr_lr, label='LR (area = {:.3f})'.format(auc_lr))
plt.plot(fpr_rf, tpr_rf, label='RF (area = {:.3f})'.format(auc_rf))
plt.plot(fpr_svc, tpr_svc, label='SVC (area = {:.3f})'.format(auc_svc))
plt.plot(fpr_nv, tpr_nv, label='NVB (area = {:.3f})'.format(auc_nv))
plt.xlabel('False positive rate')
plt.ylabel('True positive rate')
plt.title('ROC curve')
plt.legend(loc='best')
plt.show()

# Zoom in view of the upper left corner.
plt.figure(2)
plt.xlim(0, 0.2)
plt.ylim(0.8, 1)
plt.plot([0, 1], [0, 1], 'k--')
plt.plot(fpr_keras, tpr_keras, label='Keras (area = {:.3f})'.format(auc_keras))
plt.plot(fpr_lgbmfull, tpr_lgbmfull, label='LGBM (area = {:.3f})'.format(auc_lgbmfull))
plt.plot(fpr_lr, tpr_lr, label='LR (area = {:.3f})'.format(auc_lr))
plt.plot(fpr_rf, tpr_rf, label='RF (area = {:.3f})'.format(auc_rf))
plt.plot(fpr_svc, tpr_svc, label='SVC (area = {:.3f})'.format(auc_svc))
plt.plot(fpr_nv, tpr_nv, label='NVB (area = {:.3f})'.format(auc_nv))
plt.xlabel('False positive rate')
plt.ylabel('True positive rate')
plt.title('ROC curve (zoomed in at top left)')
plt.legend(loc='best')
plt.show()

